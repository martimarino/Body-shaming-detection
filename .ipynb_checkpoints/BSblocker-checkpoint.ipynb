{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91c1e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22948,
     "status": "ok",
     "timestamp": 1657314632499,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "1c91c1e7",
    "outputId": "babc71ee-06e9-4e31-ef79-7c8356e1e96b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec23085",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1657314669610,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "6ec23085",
    "outputId": "bd891756-f14c-4bf2-f9d3-ece2caaebf82"
   },
   "outputs": [],
   "source": [
    "# % cd '/content/drive/MyDrive/Data Mining/BSblocker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e3eef",
   "metadata": {
    "id": "3f5e3eef"
   },
   "source": [
    "# Scraping tweets containing keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8274c5",
   "metadata": {
    "id": "9c8274c5"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# !pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93a7c3",
   "metadata": {
    "id": "2c93a7c3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "def getFilteredTweets():\n",
    "    text_query = '(\"grasso\" OR \"grassa\" ' \\\n",
    "                 'OR \"ciccione\" ' \\\n",
    "                 'OR \"culone\" OR \"nano\" ' \\\n",
    "                 'OR \"nana\" OR \"obeso\" ' \\\n",
    "                 'OR \"pelata\" OR \"pelato\")'\n",
    "    since_date = '2022-06-01'\n",
    "    until_date = '2022-07-01'\n",
    "    options = '-is:retweet -is:reply -is:quoted lang:it'\n",
    "    output_file = './raw_Scraped/2022-06.csv'\n",
    "    tweets_list = []\n",
    "    fetched = 0\n",
    "\n",
    "    print(\"output_file: \", output_file)\n",
    "\n",
    "    for i, tweet in enumerate(\n",
    "            sntwitter.TwitterSearchScraper(\n",
    "                text_query + ' since:' + since_date + ' until:' + until_date + ' ' + options).get_items()):\n",
    "        dfk = pd.read_csv(\"keys.txt\", sep=';')\n",
    "\n",
    "        Words = dfk['Words'].values\n",
    "        for word in Words:\n",
    "            if word in tweet.content:\n",
    "\n",
    "                fetched = fetched+1\n",
    "\n",
    "                tweets_list.append(\n",
    "                    [tweet.date, tweet.content, tweet.user.username])\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Tweets obtained: \", fetched, \"\\t\\tdate: \", tweet.date)\n",
    "\n",
    "                break\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list,\n",
    "                             columns=['datetime', 'text', 'username'])\n",
    "    tweets_df.to_csv(output_file, index=False, sep=',')\n",
    "\n",
    "\n",
    "def scrape():\n",
    "    getFilteredTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0733939",
   "metadata": {
    "id": "b0733939"
   },
   "source": [
    "\n",
    "# Clean data before labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcbb3e2",
   "metadata": {
    "id": "5dcbb3e2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean():\n",
    "\n",
    "    input_file = './raw_scraped/2022-06.csv'\n",
    "    output_file = './cleaned/2022-06-cleaned.csv'\n",
    "\n",
    "    # remove dup\n",
    "    df = pd.read_csv(input_file, index_col=False, delimiter=\",\")\n",
    "    df.drop_duplicates(subset=['text', 'username'])\n",
    "\n",
    "    dfk = pd.read_csv(\"./keys.txt\", sep=';')\n",
    "    Words = dfk['Words'].values\n",
    "\n",
    "    tweets_list = []\n",
    "    how_many = 0\n",
    "    found = 0\n",
    "\n",
    "\n",
    "    # insert 'target' column\n",
    "    df.insert(3, 'target', \" \", allow_duplicates=True)\n",
    "    df.to_csv(output_file, index=False, sep=',')\n",
    "\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # remove URLs and mentions\n",
    "        df.at[i, 'text'] = re.sub(r\"(?:\\@|https?\\://)\\S+\", '', df.at[i, 'text'], flags=re.MULTILINE)\n",
    "\n",
    "        # remove new lines\n",
    "        if df.at[i, 'text'].endswith(\"\\n\") or df.at[i, 'text'].endswith(\"\\r\"):\n",
    "            df.at[i, 'text'] = df.at[i, 'text'].replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "        else:\n",
    "            df.at[i, 'text'] = df.at[i, 'text'].replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "        # remove multiple spaces\n",
    "        df.at[i, 'text'] = re.sub('\\\\s+', ' ', df.at[i, 'text'])\n",
    "\n",
    "        # remove tweets without keywords\n",
    "        for word in Words:\n",
    "\n",
    "            found = 0\n",
    "\n",
    "            if word in df.at[i, 'text'].lower():\n",
    "\n",
    "                found = 1\n",
    "\n",
    "                tweets_list.append(\n",
    "                    [df.at[i, 'datetime'], df.at[i, 'text'], df.at[i, 'username'], df.at[i, 'target']])\n",
    "\n",
    "                how_many = how_many + 1\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Tweets filtered: \", how_many, \"\\t\\tdate: \", df.at[i, 'datetime'])\n",
    "\n",
    "                break\n",
    "        if found == 0:\n",
    "            print(\"Deleted: \", df.at[i, 'text'])\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['datetime', 'text', 'username', 'target'])\n",
    "    tweets_df.to_csv(output_file, index=False, sep=',')\n",
    "    print(tweets_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990d29c",
   "metadata": {
    "id": "8990d29c"
   },
   "source": [
    "## - Remove tweets with few occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ad43f",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "829ad43f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def remove_few_occurences():\n",
    "    \n",
    "    data = pd.read_csv(\"./cleaned/2021-12-cleaned.csv\")\n",
    "    white_list = [\"pelato\", \"pelata\", \"nano\", \"nana\", \"obeso\", \"obesa\", \"cozza\",\n",
    "                 \"ciccione\", \"grasso\", \"grassa\"]\n",
    "    black_list = [\"boiler\", \"anoressica\", \"anoressico\", \"cicciona\", \"nasone\", \n",
    "                 \"racchia\", \"culona\", \"obesa\"]\n",
    "\n",
    "    tweets_list = []\n",
    "    black = False \n",
    "    white = False\n",
    "    deleted = 0\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        if any(word in data.at[i, 'Text'].lower() for word in black_list):\n",
    "            black = True\n",
    "        if any(word in data.at[i, 'Text'].lower() for word in white_list):\n",
    "            white = True\n",
    "\n",
    "        if black and not white:\n",
    "            deleted+=1\n",
    "            #print(deleted)\n",
    "            #print(data.at[i, 'Text'])\n",
    "            continue\n",
    "        else:\n",
    "            tweets_list.append(\n",
    "                [data.at[i, 'Datetime'], data.at[i, 'Text'], \n",
    "                 data.at[i, 'Username']])   \n",
    "\n",
    "            #tweets_df.to_csv(\"./labeled/prova-2.csv\", index=False, sep=',')\n",
    "        black = False \n",
    "        white = False\n",
    "\n",
    "    print(tweets_df.shape)\n",
    "    print(deleted)\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text', 'Username'])\n",
    "    tweets_df.to_csv(\"./cleaned/2021-12-filtered.csv\", index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086c6c0",
   "metadata": {
    "id": "5086c6c0"
   },
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f41d9",
   "metadata": {
    "id": "ec8f41d9"
   },
   "source": [
    "## - Concat more files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c411904",
   "metadata": {
    "id": "8c411904"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge():\n",
    "    # setting the path for joining multiple files\n",
    "    files = os.path.join(\"./monitoring/\", \"*.csv\")\n",
    "\n",
    "    # list of merged files returned\n",
    "    files = glob.glob(files)\n",
    "\n",
    "    print(\"Resultant CSV after joining all CSV files at a particular location...\");\n",
    "\n",
    "    # joining files with concat and read_csv\n",
    "    df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "    df.sort_values('datetime', inplace=True, ascending=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    df.to_csv('./merged/monitoring.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb38ce",
   "metadata": {
    "id": "12bb38ce"
   },
   "source": [
    "## - Concat two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2dc052",
   "metadata": {
    "id": "9d2dc052"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv()\n",
    "data2 = pd.read_csv()\n",
    "\n",
    "concate_data = pd.concat([data1,data2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351db08",
   "metadata": {
    "id": "8351db08"
   },
   "source": [
    "# Show data collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce292406",
   "metadata": {
    "id": "ce292406"
   },
   "source": [
    "## - Set filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d39926",
   "metadata": {
    "id": "83d39926"
   },
   "outputs": [],
   "source": [
    "file_name = './labeled/12-01-rebalanced.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94862698",
   "metadata": {
    "id": "94862698"
   },
   "source": [
    "## - Details of a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d71b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1656148131071,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "f44d71b5",
    "outputId": "e5101c21-c98b-4509-af07-45729cbdc435"
   },
   "outputs": [],
   "source": [
    "# pip install pandas \n",
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "# Print it out if you want\n",
    "print(data.shape)\n",
    "data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BjVW0mYBEMMH",
   "metadata": {
    "id": "BjVW0mYBEMMH"
   },
   "source": [
    "## - Class plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WImPB-AtERf1",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1657188919410,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "WImPB-AtERf1",
    "outputId": "eaeae521-8260-4f70-814c-26cdd6cc0eb3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./labeled/12-01-rebalanced-only-labeled.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "data['target'].value_counts().plot(kind='bar')\n",
    "plt.xlabel(\"Label\", labelpad=14)\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel(\"Tweets\", labelpad=14);\n",
    "# plt.title(\"Training set\", y=1.02);\n",
    "\n",
    "plt.savefig('training set.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dae0b5",
   "metadata": {
    "id": "07dae0b5"
   },
   "source": [
    "## - Histogram of a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8b8d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 682,
     "status": "error",
     "timestamp": 1657188089238,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "1ed8b8d1",
    "outputId": "5cb4078e-d93e-4073-e401-de8c16e8e7f4"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(file_name, index_col=False, delimiter=\",\")\n",
    "\n",
    "# get only month and year from datetime column (date of exam)\n",
    "df['date_col'] = df['Datetime'].astype('datetime64').dt.to_period('D') # D = day, M = month, Y = year\n",
    "print(df.shape)\n",
    "\n",
    "# group by based on month and year after filtering poor graded students\n",
    "data = df.groupby(['date_col']).size().reset_index(name = 'count')  \n",
    "\n",
    "ax = sns.barplot(x=\"date_col\", y=\"count\", data=data, zorder=2).set(title='Distribution of tweets') #plot using seaborn\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()\n",
    "plt.rcParams[\"figure.figsize\"] = [11,9]\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bcca8d",
   "metadata": {
    "id": "89bcca8d"
   },
   "source": [
    "## - Bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425c1d1",
   "metadata": {
    "id": "3425c1d1"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "df = pd.read_csv('./merged/monitoring.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "# get only month and year from datetime column (date of exam)\n",
    "df['date_col'] = df['datetime'].astype('datetime64').dt.to_period('D') # D = day, M = month, Y = year\n",
    "print(df.shape)\n",
    "\n",
    "# group by based on month and year after filtering poor graded students\n",
    "data = df.groupby(['date_col']).size().reset_index(name = 'count')  \n",
    "\n",
    "ax = sns.barplot(x=\"date_col\", y=data[\"count\"], data=data, zorder=2, color=\"cornflowerblue\")\n",
    "# plt.title('Distribution of tweets',fontsize=36,pad=30)\n",
    "plt.xlabel('Time',fontsize=30,labelpad=24)\n",
    "plt.ylabel('#Tweets',fontsize=30)\n",
    "plt.xticks(rotation=90, fontsize=16)\n",
    "plt.yticks(fontsize=26)\n",
    "plt.tight_layout()\n",
    "plt.rcParams[\"figure.figsize\"] = [28,9]\n",
    "plt.grid()\n",
    "plt.savefig('./monitoring/monitoring_barplot_days.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0008c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./merged/monitoring.csv', index_col=False, delimiter=\",\")\n",
    "# get only month and year from datetime column (date of exam)\n",
    "df['date_col'] = df['datetime'].astype('datetime64').dt.to_period('D') # D = day, M = month, Y = year\n",
    "print(df.shape)\n",
    "\n",
    "# group by based on month and year after filtering poor graded students\n",
    "data = df.groupby(['date_col']).size().reset_index(name = 'count')  \n",
    "data['date_col'] = data['date_col'].dt.to_timestamp('s').dt.strftime('%Y-%m-%d')\n",
    "data['date_col'] = pd.to_datetime(data['date_col'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Set the locator\n",
    "locator = mdates.MonthLocator()  # every month\n",
    "# Specify the format - %b gives us Jan, Feb...\n",
    "fmt = mdates.DateFormatter('%b')\n",
    "# Plot\n",
    "plt.bar(data['date_col'], data['count'], color=\"cornflowerblue\")\n",
    "X = plt.gca().xaxis\n",
    "X.set_major_locator(locator)\n",
    "# Specify formatter\n",
    "X.set_major_formatter(fmt)\n",
    "\n",
    "# Labels\n",
    "# plt.title('Distribution of tweets',fontsize=36,pad=30)\n",
    "# plt.xlabel('Time',fontsize=30,labelpad=50)\n",
    "plt.ylabel('#Tweets',fontsize=36,labelpad=26)\n",
    "#Ticks\n",
    "plt.xticks(rotation=90, fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.tick_params(axis='y', pad=10)\n",
    "plt.tick_params(axis='x', pad=10)\n",
    "\n",
    "plt.margins(x=0)    # delete graph lateral margin\n",
    "plt.ylim(ymin=0)    # y values start from 0\n",
    "plt.tight_layout()\n",
    "plt.rcParams[\"figure.figsize\"] = [28,9]\n",
    "plt.grid(zorder=2)\n",
    "plt.savefig('./monitoring/monitoring_barplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612585e6",
   "metadata": {},
   "source": [
    "## - Find peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "input_file = './merged/cleaned.csv'\n",
    "dataset = pd.read_csv(input_file)\n",
    "dataset = dataset.groupby(dataset['datetime'].astype('datetime64').dt.to_period('d')).size().reset_index(name='counts')\n",
    "dataset[\"datetime\"]= dataset[\"datetime\"].dt.strftime('%Y-%m-%d')\n",
    "dataset = dataset.iloc[:, ::-1]\n",
    "\n",
    "res = dataset[(dataset['counts'] > 800) & (dataset['datetime'] > '2022-03-01')]\n",
    "print(res)\n",
    "\n",
    "peaks = {}\n",
    "for index, row in res.iterrows():\n",
    "    peaks[row['datetime']] = row['counts']\n",
    "print(peaks)\n",
    "Discarded = ['2022-03-14', '2022-03-19', '2022-03-24']\n",
    "for peak in Discarded:\n",
    "    peaks.pop(peak)\n",
    "print(peaks)\n",
    "\n",
    "ax = plt.plot(dataset['datetime'], dataset['counts'])\n",
    "plt.xticks(rotation=70)\n",
    "plt.tight_layout()\n",
    "plt.rcParams[\"figure.figsize\"] = [50,20]\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e226d",
   "metadata": {},
   "source": [
    "## - Peaks highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17893aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./merged/monitoring.csv', index_col=False, delimiter=\",\")\n",
    "# get only month and year from datetime column (date of exam)\n",
    "df['date_col'] = df['datetime'].astype('datetime64').dt.to_period('D') # D = day, M = month, Y = year\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "# group by based on month and year after filtering poor graded students\n",
    "data = df.groupby(['date_col']).size().reset_index(name = 'count')  \n",
    "data['date_col'] = data['date_col'].dt.to_timestamp('s').dt.strftime('%Y-%m-%d')\n",
    "data['date_col'] = pd.to_datetime(data['date_col'], format='%Y-%m-%d')\n",
    "data.set_index(\"date_col\")\n",
    "\n",
    "# Set the locator\n",
    "locator = mdates.MonthLocator()  # every month\n",
    "# Specify the format - %b gives us Jan, Feb...\n",
    "fmt = mdates.DateFormatter('%b')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# bars = ax.bar(x=\"date_col\", y=data[\"count\"], data=data, zorder=2, color=\"cornflowerblue\")\n",
    "\n",
    "barlist = plt.bar(data['date_col'], data['count'], color=\"cornflowerblue\")\n",
    "\n",
    "X = plt.gca().xaxis\n",
    "X.set_major_locator(locator)\n",
    "# Specify formatter\n",
    "X.set_major_formatter(fmt)\n",
    "# Labels\n",
    "# plt.title('Distribution of tweets',fontsize=36,pad=30)\n",
    "# plt.xlabel('Time',fontsize=30,labelpad=50)\n",
    "plt.ylabel('#Tweets',fontsize=36,labelpad=26)\n",
    "#Ticks\n",
    "plt.xticks(rotation=90, fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.tick_params(axis='y', pad=10)\n",
    "plt.tick_params(axis='x', pad=10)\n",
    "\n",
    "plt.margins(x=0)    # delete graph lateral margin\n",
    "plt.ylim(ymin=0)    # y values start from 0\n",
    "plt.tight_layout()\n",
    "plt.rcParams[\"figure.figsize\"] = [28,9]\n",
    "plt.grid(zorder=2)\n",
    "# plt.subplots_adjust(bottom=0.55)\n",
    "\n",
    "print(peaks)\n",
    "\n",
    "for peak in peaks.keys():\n",
    "    print(peak)\n",
    "    p = datetime.strptime(peak, '%Y-%m-%d')\n",
    "    i = data[data['date_col'] == peak].index[0]\n",
    "    row = data[data['date_col'] == peak]\n",
    "    ts = row.iloc[0]['date_col']\n",
    "    s = ts.strftime('%Y-%m-%d')\n",
    "    plt.text(ts,-300, s, fontsize=24, rotation=90)\n",
    "    barlist[i].set_color('salmon')\n",
    "    \n",
    "\n",
    "plt.savefig('./monitoring/img/monitoring_barplot_peaks.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafaae0",
   "metadata": {},
   "source": [
    "## - Line chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e05a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('./merged/monitoring.csv', index_col=False, delimiter=\",\")\n",
    "# get only month and year from datetime column (date of exam)\n",
    "df['date_col'] = df['datetime'].astype('datetime64').dt.to_period('D') # D = day, M = month, Y = year\n",
    "print(df.shape)\n",
    "\n",
    "# group by based on month and year after filtering poor graded students\n",
    "data = df.groupby(['date_col']).size().reset_index(name = 'count')  \n",
    "data['date_col'] = data['date_col'].dt.to_timestamp('s').dt.strftime('%Y-%m-%d')\n",
    "data['date_col'] = pd.to_datetime(data['date_col'], format='%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Set the locator\n",
    "locator = mdates.MonthLocator()  # every month\n",
    "# Specify the format - %b gives us Jan, Feb...\n",
    "fmt = mdates.DateFormatter('%b')\n",
    "\n",
    "plt.plot(data['date_col'], data['count'], color=\"cornflowerblue\")\n",
    "X = plt.gca().xaxis\n",
    "X.set_major_locator(locator)\n",
    "# Specify formatter\n",
    "X.set_major_formatter(fmt)\n",
    "# Labels\n",
    "# plt.title('Distribution of tweets',fontsize=36,pad=30)\n",
    "# plt.xlabel('Time',fontsize=30,labelpad=50)\n",
    "plt.ylabel('#Tweets',fontsize=36,labelpad=26)\n",
    "#Ticks\n",
    "plt.xticks(rotation=90, fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.tick_params(axis='y', pad=10)\n",
    "plt.tick_params(axis='x', pad=10)\n",
    "\n",
    "plt.margins(x=0)    # delete graph lateral margin\n",
    "plt.ylim(ymin=0)    # y values start from 0\n",
    "plt.tight_layout()\n",
    "plt.rcParams[\"figure.figsize\"] = [28,9]\n",
    "plt.grid()\n",
    "plt.savefig('./monitoring/monitoring_lineplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62310e",
   "metadata": {
    "id": "da62310e"
   },
   "source": [
    "## - Frequency of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111ede4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1656148425701,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "7111ede4",
    "outputId": "a9b0c05c-960d-414a-8f01-0f2ff090cfca"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('./labeled/12-01-rebalanced.csv')\n",
    "\n",
    "dfa = df['text']\n",
    "dfb = pd.read_csv(\"keys.txt\", sep=';')\n",
    "\n",
    "Words = dfb['Words'].values\n",
    "dico = {}\n",
    "for word in Words:\n",
    "    dico[word] = dfa.str.count(word).sum()\n",
    "\n",
    "print(dico)\n",
    "\n",
    "#plt.bar(dico.keys(), dico.values(), width, color='g')\n",
    "\n",
    "words = list(dico.keys())\n",
    "count = list(dico.values())\n",
    "    \n",
    "plt.barh(words,count)\n",
    "plt.title('Word occurences')\n",
    "plt.ylabel('Keywords')\n",
    "plt.xlabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc79b2e",
   "metadata": {
    "id": "2bc79b2e"
   },
   "source": [
    "## - Extract labeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef681c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1657269750812,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "5ef681c5",
    "outputId": "68c34802-3d99-4182-a61e-adf5fa6cded6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_labeled(in_file, out_file):\n",
    "\n",
    "    df = pd.read_csv(in_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "    tweets_list = []\n",
    "    print(df.shape)\n",
    "\n",
    "    labeled = df[(df.target == '0') | (df.target == '1')]\n",
    "    labeled.to_csv(out_file, index=False, sep=',')\n",
    "\n",
    "    return (labeled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C45M-Rribw5C",
   "metadata": {
    "id": "C45M-Rribw5C"
   },
   "source": [
    "## - Verify balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9w5O-BtVbwB5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1656925854573,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "9w5O-BtVbwB5",
    "outputId": "0bfc6de2-e495-47e2-9f9c-483fd47822e3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./labeled/12-01-rebalanced-only-labeled.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "tot0 = data.query(\"target == 0\").shape[0]\n",
    "tot1 = data.query(\"target == 1\").shape[0]\n",
    "\n",
    "print(\"Tot 0 --> \", tot0)\n",
    "print(\"Tot 1 --> \", tot1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019da1ef",
   "metadata": {
    "id": "019da1ef"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0baccbd",
   "metadata": {
    "id": "d0baccbd"
   },
   "source": [
    "## - Remove punctuation marks, brackets, quotes, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fb518",
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1657314641174,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "f77fb518"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join([i for i in str(text) if i not in string.punctuation])\n",
    "    text = text.replace('\\u201D', \" \")\n",
    "    text = text.replace('\\u2018', \" \")\n",
    "    text = text.replace('\\u2019', \" \")\n",
    "    text = text.replace('\\u201c', \" \")\n",
    "    text = text.replace('\\u2026', \" \")\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b5b97",
   "metadata": {
    "id": "9e9b5b97"
   },
   "source": [
    "## - Text reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f109a",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1657314641415,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "1a8f109a"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def text_reformat(text):\n",
    "    # remove two or more dots\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    # remove two or more letters: { bellooooo -> bello}\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dde321",
   "metadata": {
    "id": "f0dde321"
   },
   "source": [
    "## - Remove emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6e650",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657314641416,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "bfc6e650"
   },
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_nADI-wV9VAS",
   "metadata": {
    "id": "_nADI-wV9VAS"
   },
   "source": [
    "## - Preprocessing call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XKK4Qc1p6ljt",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657314641416,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "XKK4Qc1p6ljt"
   },
   "outputs": [],
   "source": [
    "def preprocessing_steps(data):\n",
    "    new_data = remove_punctuation(data)\n",
    "    new_data = text_reformat(new_data)\n",
    "    new_data = remove_emojis(new_data)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "\n",
    "    array = []\n",
    "\n",
    "    for index, tweet in data.iterrows():\n",
    "\n",
    "        # print(tweet['Text'])\n",
    "        new_data = preprocessing_steps(tweet['text'])\n",
    "        array.append(new_data)\n",
    "        # print(new_data, \"\\n\")\n",
    "\n",
    "    data['text'] = array\n",
    "\n",
    "    print(\"Preprocessing done\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcT4gSRf8GDf",
   "metadata": {
    "id": "qcT4gSRf8GDf"
   },
   "source": [
    "# Elaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3659ab",
   "metadata": {
    "id": "8f3659ab"
   },
   "source": [
    "## - Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f888169",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1657314643600,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "1f888169"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('italian'))\n",
    "    \n",
    "    return [i for i in tokens if i not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22294383",
   "metadata": {
    "id": "22294383"
   },
   "source": [
    "## - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b8335",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657314643838,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "980b8335"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stem(tokens):\n",
    "    # the stemmer requires a language parameter\n",
    "    snow_stemmer = SnowballStemmer(language='italian')\n",
    "\n",
    "    return [snow_stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QiQ2PJVj8qRJ",
   "metadata": {
    "id": "QiQ2PJVj8qRJ"
   },
   "source": [
    "## - Remove miningless words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2QXZrIQ865b",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1657314643838,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "v2QXZrIQ865b"
   },
   "outputs": [],
   "source": [
    "def miningfull_words(stemmed):\n",
    "    return [word for word in stemmed if len(word) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3PdYjVs-k2u",
   "metadata": {
    "id": "h3PdYjVs-k2u"
   },
   "source": [
    "## - Remove features with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tx9QrjEz-pav",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1657314643838,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "tx9QrjEz-pav"
   },
   "outputs": [],
   "source": [
    "def remove_numbers(mean_words):\n",
    "    return [word for word in mean_words if not word.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0Y4noSc9REE",
   "metadata": {
    "id": "d0Y4noSc9REE"
   },
   "source": [
    "## - Elaboration call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dX6LQ2t9FXK",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657314643839,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "3dX6LQ2t9FXK"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def textual(text):\n",
    "    tweets = \"\"\n",
    "    for word in text:\n",
    "        tweets += word + \" \"\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def elaborating_steps(t):\n",
    "    # print(t)\n",
    "\n",
    "    tokens = word_tokenize(t)\n",
    "    # print(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    # print(tokens)\n",
    "    stemmed_words = stem(tokens)\n",
    "    # print(stemmed_words)\n",
    "    mean_words = miningfull_words(stemmed_words)\n",
    "    # print(mean_words)\n",
    "    numbers_removed = remove_numbers(mean_words)\n",
    "    # print(numbers_removed)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    elaborated_tweet = \"\"\n",
    "    for word in numbers_removed:\n",
    "        elaborated_tweet += word + \" \"\n",
    "\n",
    "    return elaborated_tweet\n",
    "\n",
    "\n",
    "def elaborate(data):\n",
    "\n",
    "    elaborated = []\n",
    "\n",
    "    for index, tweet in data.iterrows():\n",
    "\n",
    "        # print(tweet['text'])\n",
    "        new_tweet = elaborating_steps(tweet['text']).strip()\n",
    "        elaborated.append(new_tweet)\n",
    "        data.at[index, 'text'] = new_tweet\n",
    "\n",
    "    # print(elaborated)\n",
    "    data = data.sort_values(by='datetime')\n",
    "\n",
    "    print(\"Elaboration done\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061c9ba",
   "metadata": {
    "id": "7061c9ba"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I8c7087a0E4I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18097,
     "status": "ok",
     "timestamp": 1657319393079,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "I8c7087a0E4I",
    "outputId": "2d75be9e-4726-46b9-ec50-fca052f7844d"
   },
   "outputs": [],
   "source": [
    "!pip install dataframe_image\n",
    "!pip install --upgrade pip\n",
    "!pip install selenium\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nZ3mdRyECEjG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "executionInfo": {
     "elapsed": 1301,
     "status": "error",
     "timestamp": 1657319399628,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "nZ3mdRyECEjG",
    "outputId": "7857d6bb-8d25-4c41-ba09-df76cd1c25d2"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "\n",
    "# import classifiers\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# model selection and metrics\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# import plot libs\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import dataframe_image as dfi\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#from selenium import webdriver\n",
    "#driver = webdriver.Chrome(\"C:/Users/marti/Downloads/chromedriver_win32/chromedriver.exe\")\n",
    "\n",
    "\n",
    "rounds = 10\n",
    "folds = 10\n",
    "perc = \"85\"\n",
    "path = \"models_result/\"+perc+\"/\"\n",
    "\n",
    "\n",
    "def t_stat_interpret(t):\n",
    "    \"\"\"\n",
    "    Takes a scalar and returns a string with\n",
    "    the css property `'color: yellow'` for queue values, white otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # degrees of freedom\n",
    "    p = 0.05\n",
    "    df = rounds - 1\n",
    "    t_table = pd.read_csv(\"./t_distribution_table.csv\")\n",
    "    c = float(t_table.loc[df, str(round(p / 2, 3))])\n",
    "\n",
    "    if t == \"\":\n",
    "        color = 'white'\n",
    "    else:\n",
    "        #color = 'white' if t > c or t < -c else 'yellow'\n",
    "        color = 'pink' if t > c or t < -c else 'lightgreen'\n",
    "    return 'background: % s' % color\n",
    "\n",
    "\n",
    "def scoring(pipeline, data, labels, iter):\n",
    "    results_10CV = []\n",
    "\n",
    "    # start iter\n",
    "    for i in range(1, iter + 1):\n",
    "        X, y = shuffle(data, labels, random_state=i * 42)\n",
    "        results_10CV.append(np.mean(cross_val_score(estimator=pipeline,\n",
    "                                                    X=X,\n",
    "                                                    y=y,\n",
    "                                                    cv=10,\n",
    "                                                    n_jobs=-1\n",
    "                                                    )))\n",
    "\n",
    "    return results_10CV\n",
    "\n",
    "\n",
    "# ------------------------ 10-fold cross validation ------------------------\n",
    "\n",
    "def cross_validation(models, tweets, targets):\n",
    "\n",
    "    # properties for new dataframe\n",
    "    idx = (model['name'] for model in models)\n",
    "    cols = ['Accuracy', 'Execution time', 'Std']\n",
    "    cvs = pd.DataFrame(np.zeros((11, 3)), columns=cols, index=idx)\n",
    "    # cm variable by the color palette from seaborn\n",
    "    cm = sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "    cs = sns.light_palette(\"royalblue\", as_cmap=True)\n",
    "\n",
    "    for model in models:\n",
    "        start = time.time()\n",
    "\n",
    "        model['pipeline'] = Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "                                            ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True)),\n",
    "                                            ('fselect', SelectPercentile(chi2, percentile=int(perc))),\n",
    "                                            ('clf', model['fun'])])\n",
    "\n",
    "        model['values'] = scoring(model['pipeline'], tweets, targets, rounds)\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        cvs.loc[model['name']] = [float((\"%.6f\" % np.mean(model['values'])).rstrip('0').rstrip('.')), \\\n",
    "                                  str(round((end - start),4))+\" s\", \\\n",
    "                                  float(\"%.6f\" % np.std(model['values']))]\n",
    "        cvs.sort_values('Accuracy', inplace=True, ascending=False)\n",
    "\n",
    "    print(\"\\nCross validation results:\\n\")\n",
    "    cvs.to_csv(path+\"training_result_\"+perc+\"/cross_val_result.csv\")\n",
    "    \n",
    "    cvs = cvs.style.background_gradient(cmap=cm, subset=['Accuracy'])\\\n",
    "                   .background_gradient(cmap=cs, subset=['Std'])\n",
    "    dfi.export(cvs, path+\"training_result_\"+perc+\"/cross_val_result.png\")\n",
    "    display(HTML(cvs.to_html()))\n",
    "\n",
    "    # discarded for execution time\n",
    "    discarded = [\"Bagging\", \"Random Forest\", \"Gradient Boosting\"]\n",
    "    # discarded for accuracy\n",
    "    discarded.extend([\"K Nearest\", \"Decision Tree\", \"Ada Boost\", \\\n",
    "                      \"Stochastic Gradient\"])\n",
    "    return discarded\n",
    "\n",
    "# ----------------- t-test evaluation from library -------------------------\n",
    "\n",
    "def t_test(models_selected):\n",
    "\n",
    "    all_t_stat = []\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for model in models_selected:\n",
    "        row = []\n",
    "        i += 1\n",
    "        j = 0\n",
    "        for another_model in models_selected:\n",
    "            j += 1\n",
    "            if (j < i + 1):\n",
    "                row.append(\"\")\n",
    "                continue\n",
    "            t_statistic, p_value = stats.ttest_rel(model['values'], \\\n",
    "                                                   another_model['values'])\n",
    "            # print(t_statistic, p_value)\n",
    "            row.append(t_statistic)\n",
    "\n",
    "        all_t_stat.append(row)\n",
    "\n",
    "    print(\"\\nT-test results:\\n\")\n",
    "\n",
    "    ttest_matrix = pd.DataFrame(all_t_stat, \n",
    "                                columns=(model['name'] for model in models_selected),\n",
    "                                index=(model['name'] for model in models_selected))\n",
    "    #delete empty column and row\n",
    "    del ttest_matrix['Logistic Regression']\n",
    "    ttest_matrix.drop(ttest_matrix.tail(1).index,inplace=True)\n",
    "    \n",
    "    ttest_matrix.to_csv(path+\"training_result_\"+perc+\"/t_test_result.csv\")\n",
    "    ttest_matrix = ttest_matrix.style.applymap(t_stat_interpret)\n",
    "    dfi.export(ttest_matrix, path+\"training_result_\"+perc+\"/t_test_result.png\")\n",
    "    display(HTML(ttest_matrix.to_html()))\n",
    "\n",
    "    discarded = []\n",
    "    discarded.append(\"MultinomialNB\")\n",
    "\n",
    "    return discarded\n",
    "\n",
    "\n",
    "# ---------------------- report and confusion matrix ----------------------- \n",
    "\n",
    "def get_report_conf_matrix(models, tweets, targets):\n",
    "\n",
    "    print(\"\\nReport and confusion matrix\")\n",
    "\n",
    "    for model in models:\n",
    "        \n",
    "        print(model['name'])\n",
    "        predict = cross_val_predict(model['pipeline'], tweets, targets, cv=10)\n",
    "        rep = metrics.classification_report(targets, predict,\n",
    "                                            target_names=['0', '1'])\n",
    "        print(rep)\n",
    "      \n",
    "        # save report\n",
    "        rep = metrics.classification_report(targets, predict,\n",
    "                                    target_names=['0', '1'], output_dict=True)\n",
    "        df = pd.DataFrame(rep)\n",
    "        df.to_csv(path+'training_result_'+perc+'/'+model['name']+'-report.csv')\n",
    "        \n",
    "        # calculate and print confusion matrix\n",
    "        disp = ConfusionMatrixDisplay.from_predictions(\n",
    "            targets,\n",
    "            predict,\n",
    "            values_format='g',\n",
    "            display_labels=[0,1],\n",
    "            cmap=plt.cm.Blues\n",
    "        )\n",
    "        disp.ax_.set_title(\"Confusion matrix\")\n",
    "\n",
    "        print(disp.confusion_matrix)\n",
    "        disp.figure_.savefig(path+'training_result_'+perc+'/'+model['name']+'-confusion_matrix.png')\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def train_models(data):\n",
    "\n",
    "    data.sample(frac=1)\n",
    "\n",
    "    tweets = data.text\n",
    "    targets = data.target\n",
    "\n",
    "\n",
    "    # models = name | fun | pipeline | values |\n",
    "    models = [\n",
    "        {\"name\": \"Logistic Regression\", \"fun\": LogisticRegression()},\n",
    "        {\"name\": \"SVM\", \"fun\": svm.SVC()},\n",
    "        {\"name\": \"Decision Tree\", \"fun\": DecisionTreeClassifier()},\n",
    "        {\"name\": \"MultinomialNB\", \"fun\": MultinomialNB()},\n",
    "        {\"name\": \"Gradient Boosting\", \"fun\": GradientBoostingClassifier()},\n",
    "        {\"name\": \"ComplementNB\", \"fun\": ComplementNB()},\n",
    "        {\"name\": \"K Nearest\", \"fun\": KNeighborsClassifier()},\n",
    "        {\"name\": \"Random Forest\", \"fun\": RandomForestClassifier()},\n",
    "        {\"name\": \"Ada Boost\", \"fun\": AdaBoostClassifier()},\n",
    "        {\"name\": \"Bagging\", \"fun\": BaggingClassifier()},\n",
    "        {\"name\": \"Stochastic Gradient\", \"fun\": SGDClassifier()}\n",
    "    ]\n",
    "\n",
    "    # analyze classifiers\n",
    "\n",
    "    discarded = cross_validation(models, tweets, targets)\n",
    "\n",
    "    models_selected = [s for s in models if s['name'] not in discarded]\n",
    "\n",
    "    discarded = t_test(models_selected)\n",
    "\n",
    "    models_selected = [s for s in models_selected if s['name'] not in discarded]\n",
    "\n",
    "    get_report_conf_matrix(models, tweets, targets)\n",
    "\n",
    "\n",
    "    # models building\n",
    "\n",
    "    for model in models_selected:\n",
    "      \n",
    "        m = model['pipeline'].fit(tweets, targets)\n",
    "        \n",
    "        # save models\n",
    "        filename = model['name'] + '.sav'\n",
    "        pickle.dump(m, open(path+'models_'+perc+'/'+filename, 'wb'))\n",
    "\n",
    "    print(\"\\nModels correctly saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57296676",
   "metadata": {
    "id": "57296676"
   },
   "source": [
    "# Training flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gfE-4WutQbSY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 906,
     "status": "ok",
     "timestamp": 1657314651911,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "gfE-4WutQbSY",
    "outputId": "34468158-c515-4c02-ec8f-0945cda94093"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluKY0xyipeX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 255946,
     "status": "error",
     "timestamp": 1657318605447,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "fluKY0xyipeX",
    "outputId": "640b7bea-c04d-45b0-b929-9e22c3f50b36",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from IPython.display import HTML\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    training_set_file = 'labeled/12-01-rebalanced-only-labeled.csv'\n",
    "    train_data = pd.read_csv(training_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "    train_data = preprocess(train_data)\n",
    "    train_data = elaborate(train_data)\n",
    "\n",
    "    # print(train_data.shape)\n",
    "    #tot0 = len(train_data[train_data.target == '0'])\n",
    "    #tot1 = len(train_data[train_data.target == '1'])\n",
    "\n",
    "    train_models(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XsIMhLG8hhB6",
   "metadata": {
    "id": "XsIMhLG8hhB6"
   },
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qqFqE1YUZ-zk",
   "metadata": {
    "executionInfo": {
     "elapsed": 716,
     "status": "ok",
     "timestamp": 1657294606236,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "qqFqE1YUZ-zk"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "\n",
    "# import classifiers\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# model selection and metrics\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# import plot libs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def test_models(data):\n",
    "\n",
    "    tweets = data['text']\n",
    "    targets = data['target']\n",
    "\n",
    "    models = [\n",
    "        {\"name\": \"Logistic Regression\", \"fun\": LogisticRegression()},\n",
    "        {\"name\": \"SVM\", \"fun\": svm.SVC()},\n",
    "        {\"name\": \"ComplementNB\", \"fun\": ComplementNB()}\n",
    "    ]\n",
    "\n",
    "    # load the model from disk\n",
    "    for model in models:\n",
    "\n",
    "        loaded_model = pickle.load(open(path+'models_'+perc+'/'+model['name']+'.sav', 'rb'))\n",
    "        print(model['name'])\n",
    "        score = loaded_model.score(tweets, targets)\n",
    "        print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "        y_predict = loaded_model.predict(tweets)\n",
    "\n",
    "        rep = classification_report(targets, y_predict,\n",
    "                                              target_names=['0', '1'])\n",
    "        print(rep, '\\n')\n",
    "\n",
    "        # save reports\n",
    "        rep = classification_report(targets, y_predict,\n",
    "                                    target_names=['0', '1'], output_dict=True)\n",
    "        df = pd.DataFrame(rep).transpose()\n",
    "        df.to_csv(path+'test_result_'+perc+'/'+model['name']+'-report.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hfvNWuQQ-xfk",
   "metadata": {
    "id": "hfvNWuQQ-xfk"
   },
   "source": [
    "# Test flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F2Y6-W7B-2SH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1657293702570,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "F2Y6-W7B-2SH",
    "outputId": "dc95ec0e-5e32-44df-8c7f-d83dc4cb4782"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from IPython.display import HTML\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    test_set_file = './labeled/2022-02-labeled-only.csv'\n",
    "    test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "    \n",
    "    test_data = preprocess(test_data)\n",
    "    test_data = elaborate(test_data)\n",
    "\n",
    "    # print(test_data.shape)\n",
    "    #tot0 = len(train_data[test_data.target == '0'])\n",
    "    #tot1 = len(train_data[test_data.target == '1'])\n",
    "\n",
    "    test_models(test_data)\n",
    "    #we consider just yellow -> null hp non rejected -> similar to each other -> best Complement e Logistic? Migliori accuracy e execution time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afefbb",
   "metadata": {},
   "source": [
    "# Online monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17116931",
   "metadata": {},
   "source": [
    "## - Extract peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6461cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# 2022-03-10\n",
    "# 2022-04-09\n",
    "# 2022-06-18\n",
    "\n",
    "df = pd.read_csv('./cleaned/2022-03-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "end_peak = '2022-03-10 23:59:59+00:00'\n",
    "start_peak = '2022-03-10 00:00:00+00:00'\n",
    "start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "peak.to_csv('./monitoring/period1.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "df = pd.read_csv('./cleaned/2022-04-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "end_peak = '2022-04-09 23:59:59+00:00'\n",
    "start_peak = '2022-04-09 00:00:00+00:00'\n",
    "start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "peak.to_csv('./monitoring/period2.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "df = pd.read_csv('./cleaned/2022-06-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "end_peak = '2022-06-18 23:59:59+00:00'\n",
    "start_peak = '2022-06-18 00:00:00+00:00'\n",
    "start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "peak.to_csv('./monitoring/period4.csv', index=False, sep=',')\n",
    "\n",
    "print(\"Peaks extracted \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68b40d",
   "metadata": {},
   "source": [
    "# Concept drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86920439",
   "metadata": {},
   "source": [
    "## - Static model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# model selection and metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "path = \"models_result/85/\"\n",
    "res = './monitoring/concept_drift/static/'\n",
    "\n",
    "def static_cd_test(data, peak):\n",
    "\n",
    "    tweets = data['text']\n",
    "    targets = data['target']\n",
    "\n",
    "    model = {'name': 'ComplementNB', 'fun': ComplementNB()}\n",
    " \n",
    "    loaded_model = pickle.load(open(path+'models_85/'+model['name']+'.sav', 'rb'))\n",
    "    print(model['name'])\n",
    "    score = loaded_model.score(tweets, targets)\n",
    "    print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "    y_predict = loaded_model.predict(tweets)\n",
    "\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                          target_names=['0', '1'])\n",
    "    print(rep, '\\n')\n",
    "\n",
    "    # save reports\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                target_names=['0', '1'], output_dict=True)\n",
    "    df = pd.DataFrame(rep).transpose()\n",
    "    if peak == 'may':\n",
    "        df.to_csv(res+peak+'-report.csv')\n",
    "    else:\n",
    "        df.to_csv(res+'period'+peak+'-report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fe2eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    peaks = ['period-1-labeled',\n",
    "             'period-2-labeled',\n",
    "             'period-3-labeled',\n",
    "             'period-4-labeled']\n",
    "    \n",
    "    i = 0\n",
    "    for peak in peaks:\n",
    "        \n",
    "        i+=1\n",
    "        \n",
    "        #lab = extract_labeled('./labeled/'+peak+'.csv', './labeled/'+peak+'-only.csv')\n",
    "        #print(lab)\n",
    "        test_set_file = './monitoring/'+peak+'-only.csv'\n",
    "        test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "        test_data = preprocess(test_data)\n",
    "        test_data = elaborate(test_data)\n",
    "\n",
    "        if(peak == peaks[2]):\n",
    "            static_cd_test(test_data, 'may')\n",
    "            i = 2\n",
    "            continue\n",
    "        else:\n",
    "            print(i)\n",
    "            static_cd_test(test_data, str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcc90a",
   "metadata": {},
   "source": [
    "## - Training function for sliding and incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectPercentile\n",
    "\n",
    "def cd_training(path, data, peak, c):\n",
    "\n",
    "    tweets = data.text\n",
    "    targets = data.target\n",
    "\n",
    "    model = {'name': 'ComplementNB', 'fun': ComplementNB()}\n",
    "\n",
    "    # model building\n",
    "    model['pipeline'] = Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "                                ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True)),\n",
    "                                ('fselect', SelectPercentile(chi2, percentile=85)),\n",
    "                                #('fselect', SelectKBest(chi2, k='all')),      \n",
    "                                ('clf', model['fun'])])\n",
    "\n",
    "    m = model['pipeline'].fit(tweets, targets)\n",
    "    \n",
    "    print(len(model['pipeline']['vect'].vocabulary_))\n",
    "    \n",
    "    # save model\n",
    "    if c =='i':\n",
    "        filename = model['name']+'_period'+str(i)+'.sav'\n",
    "    elif c == 's':\n",
    "        filename = model['name']+'_slide'+str(i)+'.sav'\n",
    "    \n",
    "    pickle.dump(m, open(path+'/'+filename, 'wb'))\n",
    "\n",
    "    print(\"\\nModel correctly saved!\\n\")\n",
    "    print('─' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7d5c5",
   "metadata": {},
   "source": [
    "## - Incremental model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcfe29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# model selection and metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "path = './monitoring/concept_drift/incremental/'\n",
    "\n",
    "        \n",
    "def incremental_cd_test(path, data, i):\n",
    "\n",
    "    tweets = data['text']\n",
    "    targets = data['target']\n",
    "\n",
    "    model = {\"name\": \"ComplementNB\", \"fun\": ComplementNB()}\n",
    "    print(\"./monitoring/concept_drift/incremental/\"+model['name']+'_period'+str(i)+'.sav')\n",
    "    \n",
    "    loaded_model = pickle.load(open(path+model['name']+'_period'+str(i)+'.sav', 'rb'))\n",
    "    score = loaded_model.score(tweets, targets)\n",
    "    print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "    y_predict = loaded_model.predict(tweets)\n",
    "\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                          target_names=['0', '1'])\n",
    "\n",
    "    print(rep, '\\n')\n",
    "    # save reports\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                target_names=['0', '1'], output_dict=True)\n",
    "    \n",
    "    df = pd.DataFrame(rep).transpose()\n",
    " \n",
    "    df.to_csv(path+\"period\"+str(i+1)+'-report.csv')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4febfb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "\n",
    "path = \"./monitoring/concept_drift/incremental/\"\n",
    "external_path = \"./monitoring/\"\n",
    "\n",
    "def get_period(i, peak):\n",
    "    #match i:\n",
    "        #case 1:\n",
    "        print(\"peak:\" +peak)\n",
    "        if i == 0:\n",
    "            data1 = external_path+\"12-01-rebalanced-only-labeled.csv\"\n",
    "            data2 = external_path+\"2022-02-labeled-only.csv\"\n",
    "            df1 = pd.read_csv(data1)\n",
    "            df2 = pd.read_csv(data2)\n",
    "            df = pd.concat([df1,df2])\n",
    "            df.sort_values('datetime', inplace=True, ascending=True)\n",
    "            df.to_csv(path+'peak'+str(i)+\".csv\", index=False)\n",
    "            print(df.shape)\n",
    "            return df\n",
    "        elif i > 0 and i < 4:\n",
    "            data1= path+\"peak\" + str(i-1)+\".csv\"\n",
    "            data2 = external_path+peak+'-only.csv'\n",
    "            df1 = pd.read_csv(data1)\n",
    "            df2 = pd.read_csv(data2)\n",
    "            df = pd.concat([df1,df2])\n",
    "            df.sort_values('datetime', inplace=True, ascending=True)\n",
    "            df.to_csv(path+'peak'+str(i)+\".csv\", index=False)\n",
    "            print(df.shape)\n",
    "            print(data1, data2)\n",
    "            return df \n",
    "        #case _:\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    peaks = ['period-1-labeled',\n",
    "             'period-2-labeled',\n",
    "             'period-3-labeled',\n",
    "             'period-4-labeled']\n",
    "\n",
    "    i = 0\n",
    "    for peak in peaks:\n",
    "        \n",
    "        # training\n",
    "        training_set_file = get_period(i, peaks[i-1])\n",
    "        training_data = preprocess(training_set_file)\n",
    "        training_data = elaborate(training_data)\n",
    "        cd_training(path,training_set_file, peak, 'i')\n",
    "        \n",
    "        # test\n",
    "        \n",
    "        test_set_file = './monitoring/'+peak+'-only.csv'\n",
    "        test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "        test_data = preprocess(test_data)\n",
    "        test_data = elaborate(test_data)\n",
    "        \n",
    "        incremental_cd_test(path, test_data, i)\n",
    "        \n",
    "       \n",
    "        i+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df5686",
   "metadata": {},
   "source": [
    "## - Sliding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518db23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "\n",
    "# model selection and metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "    \n",
    "def sliding_cd_training(path, data, i):\n",
    "\n",
    "    tweets = data.text\n",
    "    targets = data.target\n",
    "    \n",
    "    model = {'name': 'ComplementNB', 'fun': ComplementNB()}\n",
    "\n",
    "    # model building\n",
    "    model['pipeline'] = Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "                                ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True)),\n",
    "                                ('fselect', SelectPercentile(chi2, percentile=85)),\n",
    "                                ('clf', model['fun'])])\n",
    "\n",
    "    m = model['pipeline'].fit(tweets, targets)\n",
    "    # save model\n",
    "    filename = model['name']+'_slide'+str(i)+'.sav'\n",
    "    pickle.dump(m, open(path+'/'+filename, 'wb'))\n",
    "\n",
    "    print(\"\\nModel correctly saved!\\n\")\n",
    "    print('─' * 10)\n",
    "    \n",
    "\n",
    "def sliding_cd_test(path, data, i):\n",
    "\n",
    "    tweets = data['text']\n",
    "    targets = data['target']\n",
    "\n",
    "    loaded_model = pickle.load(open(path+model['name']+'_slide'+str(i)+'.sav', 'rb'))\n",
    "    \n",
    "    score = loaded_model.score(tweets, targets)\n",
    "    print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "    y_predict = loaded_model.predict(tweets)\n",
    "\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                          target_names=['0', '1'])\n",
    "    print(rep, '\\n')\n",
    "\n",
    "    # save reports\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                target_names=['0', '1'], output_dict=True)\n",
    "    df = pd.DataFrame(rep).transpose()\n",
    "    df.to_csv(path+'slide'+str(i)+'-report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "path = './monitoring/concept_drift/sliding/'\n",
    "\n",
    "def create_slide(i, file1, file2):\n",
    "\n",
    "    df1 = pd.read_csv(file1, index_col=False, delimiter=\",\")\n",
    "    data2 = pd.read_csv(file2, index_col=False, delimiter=\",\")\n",
    "    df1.sort_values('datetime', inplace=True, ascending=True)\n",
    "    data2.sort_values('datetime', inplace=True, ascending=True)\n",
    "    \n",
    "    print(df1.shape)\n",
    "    if i == 1:\n",
    "        data1 = df1.tail(-176)\n",
    "    else:\n",
    "        data1 = df1.tail(-80)\n",
    "    print(data1.shape)\n",
    "    print(data2.shape)\n",
    "    slide = pd.concat([data1,data2]) #slide\n",
    "    print(slide.shape)\n",
    "    #slide.sort_values('datetime', inplace=True, ascending=True)\n",
    "    slide_name = path+'slide'+str(i)+'.csv'\n",
    "    slide.to_csv(slide_name, index=False)\n",
    "    \n",
    "    return slide_name, slide\n",
    "\n",
    "    \n",
    "def get_files(i):\n",
    "    \n",
    "    list = []\n",
    "    j = i-1\n",
    "    if i == 1:\n",
    "        list.append('monitoring/12-01-rebalanced-only-labeled.csv')\n",
    "        list.append('monitoring/2022-02-labeled-only.csv')\n",
    "        \n",
    "    else: # 2 3 4 5\n",
    "        list.append(path+'slide'+str(j)+'.csv')\n",
    "        list.append('monitoring/period-'+str(j)+'-labeled-only.csv')\n",
    "    \n",
    "    return list\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    model = {\"name\": \"ComplementNB\", \"fun\": ComplementNB()}\n",
    "                    \n",
    "    \n",
    "    for i in range(1,6):\n",
    "           \n",
    "        # create slide\n",
    "        \n",
    "        list = get_files(i)  # old and new tweets\n",
    "        print(list)\n",
    "        file1 = list[0]\n",
    "        file2 = list[1]\n",
    "        slide_name, slide = create_slide(i, file1, file2)\n",
    "        print(slide_name, \"\\n\")\n",
    "\n",
    "        #train on slide\n",
    "                                \n",
    "        training_data = preprocess(slide)\n",
    "        training_data = elaborate(training_data)\n",
    "        \n",
    "        cd_training(path, training_data, i, 's')\n",
    "        \n",
    "        # test on next month\n",
    "        \n",
    "        if i < 5:\n",
    "        \n",
    "            test_set_file = './labeled/period-'+str(i)+'-labeled-only.csv'\n",
    "            test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "            test_data = preprocess(test_data)\n",
    "            test_data = elaborate(test_data)\n",
    "\n",
    "            cd_test(path, test_data, i, 's') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc17cd",
   "metadata": {},
   "source": [
    "# - Comparing learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_file = './monitoring/concept_drift/'\n",
    "\n",
    "i = 0\n",
    "\n",
    "for i in range (1,5):\n",
    "    static = pd.read_csv(path_file+\"static/static\"+str(i+1)+\"-report.csv\")\n",
    "    incremental = pd.read_csv(path_file+\"incremental/incremental\"+str(i+1)+\"-report.csv\")\n",
    "    incremental = pd.read_csv(path_file+\"sliding/slide\"+str(i+1)+\"-report.csv\")\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3f5e3eef",
    "b0733939",
    "10ddf5f3",
    "8990d29c",
    "5086c6c0",
    "ec8f41d9",
    "12bb38ce",
    "8351db08",
    "ce292406",
    "94862698",
    "BjVW0mYBEMMH",
    "07dae0b5",
    "89bcca8d",
    "da62310e",
    "C45M-Rribw5C",
    "8149c9fb",
    "019da1ef",
    "d0baccbd",
    "9e9b5b97",
    "f0dde321",
    "_nADI-wV9VAS",
    "qcT4gSRf8GDf",
    "22294383",
    "QiQ2PJVj8qRJ",
    "h3PdYjVs-k2u"
   ],
   "name": "BSblocker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
