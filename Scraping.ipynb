{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91c1e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22948,
     "status": "ok",
     "timestamp": 1657314632499,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "1c91c1e7",
    "outputId": "babc71ee-06e9-4e31-ef79-7c8356e1e96b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec23085",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1657314669610,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "6ec23085",
    "outputId": "bd891756-f14c-4bf2-f9d3-ece2caaebf82"
   },
   "outputs": [],
   "source": [
    "# % cd '/content/drive/MyDrive/Data Mining/BSblocker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e3eef",
   "metadata": {
    "id": "3f5e3eef"
   },
   "source": [
    "# Scraping tweets containing keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8274c5",
   "metadata": {
    "id": "9c8274c5"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# !pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c93a7c3",
   "metadata": {
    "id": "2c93a7c3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_file:  ./raw_scraped/2022-07.csv\n",
      "Tweets obtained:  51 \t\tdate:  2022-07-20 22:03:05+00:00\n",
      "Tweets obtained:  89 \t\tdate:  2022-07-20 21:09:54+00:00\n",
      "Tweets obtained:  142 \t\tdate:  2022-07-20 20:32:52+00:00\n",
      "Tweets obtained:  197 \t\tdate:  2022-07-20 19:48:55+00:00\n",
      "Tweets obtained:  643 \t\tdate:  2022-07-20 11:25:25+00:00\n",
      "Tweets obtained:  768 \t\tdate:  2022-07-20 08:22:17+00:00\n",
      "Tweets obtained:  819 \t\tdate:  2022-07-20 06:43:31+00:00\n",
      "Tweets obtained:  868 \t\tdate:  2022-07-20 05:02:58+00:00\n",
      "Tweets obtained:  963 \t\tdate:  2022-07-19 21:38:37+00:00\n",
      "Tweets obtained:  1061 \t\tdate:  2022-07-19 18:52:05+00:00\n",
      "Tweets obtained:  1147 \t\tdate:  2022-07-19 16:47:25+00:00\n",
      "Tweets obtained:  1246 \t\tdate:  2022-07-19 14:30:32+00:00\n",
      "Tweets obtained:  1315 \t\tdate:  2022-07-19 13:10:33+00:00\n",
      "Tweets obtained:  1389 \t\tdate:  2022-07-19 11:38:14+00:00\n",
      "Tweets obtained:  1484 \t\tdate:  2022-07-19 10:03:21+00:00\n",
      "Tweets obtained:  1535 \t\tdate:  2022-07-19 08:57:37+00:00\n",
      "Tweets obtained:  1671 \t\tdate:  2022-07-18 23:20:54+00:00\n",
      "Tweets obtained:  1724 \t\tdate:  2022-07-18 21:40:15+00:00\n",
      "Tweets obtained:  1765 \t\tdate:  2022-07-18 20:59:53+00:00\n",
      "Tweets obtained:  1800 \t\tdate:  2022-07-18 20:23:43+00:00\n",
      "Tweets obtained:  2042 \t\tdate:  2022-07-18 14:57:25+00:00\n",
      "Tweets obtained:  2194 \t\tdate:  2022-07-18 11:48:04+00:00\n",
      "Tweets obtained:  2486 \t\tdate:  2022-07-17 21:53:58+00:00\n",
      "Tweets obtained:  2687 \t\tdate:  2022-07-17 17:29:25+00:00\n",
      "Tweets obtained:  2812 \t\tdate:  2022-07-17 13:02:53+00:00\n",
      "Tweets obtained:  2933 \t\tdate:  2022-07-17 09:41:52+00:00\n",
      "Tweets obtained:  3023 \t\tdate:  2022-07-17 05:58:43+00:00\n",
      "Tweets obtained:  3045 \t\tdate:  2022-07-17 01:13:06+00:00\n",
      "Tweets obtained:  3082 \t\tdate:  2022-07-16 22:38:09+00:00\n",
      "Tweets obtained:  3202 \t\tdate:  2022-07-16 18:53:56+00:00\n",
      "Tweets obtained:  3230 \t\tdate:  2022-07-16 17:56:38+00:00\n",
      "Tweets obtained:  3262 \t\tdate:  2022-07-16 16:55:16+00:00\n",
      "Tweets obtained:  3325 \t\tdate:  2022-07-16 14:38:34+00:00\n",
      "Tweets obtained:  3355 \t\tdate:  2022-07-16 13:33:31+00:00\n",
      "Tweets obtained:  3451 \t\tdate:  2022-07-16 10:37:13+00:00\n",
      "Tweets obtained:  3479 \t\tdate:  2022-07-16 09:28:37+00:00\n",
      "Tweets obtained:  3549 \t\tdate:  2022-07-16 07:11:38+00:00\n",
      "Tweets obtained:  3600 \t\tdate:  2022-07-16 00:45:23+00:00\n",
      "Tweets obtained:  3989 \t\tdate:  2022-07-15 11:53:30+00:00\n",
      "Tweets obtained:  4020 \t\tdate:  2022-07-15 10:53:19+00:00\n",
      "Tweets obtained:  4066 \t\tdate:  2022-07-15 09:57:34+00:00\n",
      "Tweets obtained:  4426 \t\tdate:  2022-07-14 17:13:55+00:00\n",
      "Tweets obtained:  5003 \t\tdate:  2022-07-13 17:43:21+00:00\n",
      "Tweets obtained:  5076 \t\tdate:  2022-07-13 15:38:59+00:00\n",
      "Tweets obtained:  5278 \t\tdate:  2022-07-13 10:09:52+00:00\n",
      "Tweets obtained:  5404 \t\tdate:  2022-07-13 01:39:23+00:00\n",
      "Tweets obtained:  5481 \t\tdate:  2022-07-12 21:30:15+00:00\n",
      "Tweets obtained:  5562 \t\tdate:  2022-07-12 19:37:36+00:00\n",
      "Tweets obtained:  5600 \t\tdate:  2022-07-12 18:34:55+00:00\n",
      "Tweets obtained:  5945 \t\tdate:  2022-07-12 09:23:41+00:00\n",
      "Tweets obtained:  5983 \t\tdate:  2022-07-12 07:56:00+00:00\n",
      "Tweets obtained:  6273 \t\tdate:  2022-07-11 18:13:37+00:00\n",
      "Tweets obtained:  6322 \t\tdate:  2022-07-11 17:24:22+00:00\n",
      "Tweets obtained:  6543 \t\tdate:  2022-07-11 11:53:46+00:00\n",
      "Tweets obtained:  6635 \t\tdate:  2022-07-11 09:26:49+00:00\n",
      "Tweets obtained:  6723 \t\tdate:  2022-07-11 06:47:29+00:00\n",
      "Tweets obtained:  6776 \t\tdate:  2022-07-10 23:05:31+00:00\n",
      "Tweets obtained:  6835 \t\tdate:  2022-07-10 20:33:44+00:00\n",
      "Tweets obtained:  6918 \t\tdate:  2022-07-10 18:17:35+00:00\n",
      "Tweets obtained:  7084 \t\tdate:  2022-07-10 12:42:36+00:00\n",
      "Tweets obtained:  7170 \t\tdate:  2022-07-10 10:06:16+00:00\n",
      "Tweets obtained:  7383 \t\tdate:  2022-07-09 21:03:28+00:00\n",
      "Tweets obtained:  7460 \t\tdate:  2022-07-09 19:04:59+00:00\n",
      "Tweets obtained:  7532 \t\tdate:  2022-07-09 17:04:36+00:00\n",
      "Tweets obtained:  7603 \t\tdate:  2022-07-09 14:23:26+00:00\n",
      "Tweets obtained:  7679 \t\tdate:  2022-07-09 11:46:39+00:00\n",
      "Tweets obtained:  7755 \t\tdate:  2022-07-09 08:46:29+00:00\n",
      "Tweets obtained:  7831 \t\tdate:  2022-07-09 03:05:38+00:00\n",
      "Tweets obtained:  7905 \t\tdate:  2022-07-08 20:19:49+00:00\n",
      "Tweets obtained:  7982 \t\tdate:  2022-07-08 17:50:27+00:00\n",
      "Tweets obtained:  8133 \t\tdate:  2022-07-08 12:59:09+00:00\n",
      "Tweets obtained:  8360 \t\tdate:  2022-07-07 23:03:05+00:00\n",
      "Tweets obtained:  8823 \t\tdate:  2022-07-07 08:39:30+00:00\n",
      "Tweets obtained:  8936 \t\tdate:  2022-07-07 02:20:55+00:00\n",
      "Tweets obtained:  9099 \t\tdate:  2022-07-06 18:38:18+00:00\n",
      "Tweets obtained:  9176 \t\tdate:  2022-07-06 16:22:05+00:00\n",
      "Tweets obtained:  9257 \t\tdate:  2022-07-06 13:57:53+00:00\n",
      "Tweets obtained:  9334 \t\tdate:  2022-07-06 11:49:38+00:00\n",
      "Tweets obtained:  9529 \t\tdate:  2022-07-06 05:44:24+00:00\n",
      "Tweets obtained:  9607 \t\tdate:  2022-07-05 21:43:57+00:00\n",
      "Tweets obtained:  9778 \t\tdate:  2022-07-05 17:02:30+00:00\n",
      "Tweets obtained:  9944 \t\tdate:  2022-07-05 11:02:54+00:00\n",
      "Tweets obtained:  10021 \t\tdate:  2022-07-05 09:09:13+00:00\n",
      "Tweets obtained:  10082 \t\tdate:  2022-07-05 06:49:41+00:00\n",
      "Tweets obtained:  10146 \t\tdate:  2022-07-04 22:11:06+00:00\n",
      "Tweets obtained:  10231 \t\tdate:  2022-07-04 19:38:54+00:00\n",
      "Tweets obtained:  10403 \t\tdate:  2022-07-04 13:54:13+00:00\n",
      "Tweets obtained:  10569 \t\tdate:  2022-07-04 07:49:19+00:00\n",
      "Tweets obtained:  10653 \t\tdate:  2022-07-03 21:56:22+00:00\n",
      "Tweets obtained:  10729 \t\tdate:  2022-07-03 19:07:02+00:00\n",
      "Tweets obtained:  10813 \t\tdate:  2022-07-03 16:27:03+00:00\n",
      "Tweets obtained:  10890 \t\tdate:  2022-07-03 14:23:50+00:00\n",
      "Tweets obtained:  10973 \t\tdate:  2022-07-03 11:33:16+00:00\n",
      "Tweets obtained:  11051 \t\tdate:  2022-07-03 08:34:46+00:00\n",
      "Tweets obtained:  11115 \t\tdate:  2022-07-03 02:47:43+00:00\n",
      "Tweets obtained:  11258 \t\tdate:  2022-07-02 17:13:05+00:00\n",
      "Tweets obtained:  11341 \t\tdate:  2022-07-02 13:55:13+00:00\n",
      "Tweets obtained:  11420 \t\tdate:  2022-07-02 11:40:19+00:00\n",
      "Tweets obtained:  11501 \t\tdate:  2022-07-02 09:03:07+00:00\n",
      "Tweets obtained:  11569 \t\tdate:  2022-07-02 00:05:05+00:00\n",
      "Tweets obtained:  11798 \t\tdate:  2022-07-01 14:06:53+00:00\n",
      "Tweets obtained:  11870 \t\tdate:  2022-07-01 12:01:00+00:00\n",
      "Tweets obtained:  11951 \t\tdate:  2022-07-01 08:53:06+00:00\n",
      "Tweets obtained:  12025 \t\tdate:  2022-07-01 04:36:45+00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "def getFilteredTweets():\n",
    "    text_query = '(\"grasso\" OR \"grassa\" ' \\\n",
    "                 'OR \"ciccione\" ' \\\n",
    "                 'OR \"culone\" OR \"nano\" ' \\\n",
    "                 'OR \"nana\" OR \"obeso\" ' \\\n",
    "                 'OR \"pelata\" OR \"pelato\")'\n",
    "    since_date = '2022-07-01'\n",
    "    until_date = '2022-07-21'\n",
    "    options = '-is:retweet -is:reply -is:quoted lang:it'\n",
    "    output_file = './raw_scraped/2022-07.csv'\n",
    "    tweets_list = []\n",
    "    fetched = 0\n",
    "\n",
    "    print(\"output_file: \", output_file)\n",
    "\n",
    "    for i, tweet in enumerate(\n",
    "            sntwitter.TwitterSearchScraper(\n",
    "                text_query + ' since:' + since_date + ' until:' + until_date + ' ' + options).get_items()):\n",
    "        dfk = pd.read_csv(\"keys.txt\", sep=';')\n",
    "\n",
    "        Words = dfk['Words'].values\n",
    "        for word in Words:\n",
    "            if word in tweet.content:\n",
    "\n",
    "                fetched = fetched+1\n",
    "\n",
    "                tweets_list.append(\n",
    "                    [tweet.date, tweet.content, tweet.user.username])\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Tweets obtained: \", fetched, \"\\t\\tdate: \", tweet.date)\n",
    "\n",
    "                break\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list,\n",
    "                             columns=['datetime', 'text', 'username'])\n",
    "    tweets_df.to_csv(output_file, index=False, sep=',')\n",
    "\n",
    "def main():\n",
    "    getFilteredTweets()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086c6c0",
   "metadata": {
    "id": "5086c6c0"
   },
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c411904",
   "metadata": {
    "id": "8c411904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultant CSV after joining all CSV files at a particular location...\n",
      "(144638, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge():\n",
    "    # setting the path for joining multiple files\n",
    "    files = os.path.join(\"./cleaned/\", \"*.csv\")\n",
    "\n",
    "    # list of merged files returned\n",
    "    files = glob.glob(files)\n",
    "\n",
    "    print(\"Resultant CSV after joining all CSV files at a particular location...\");\n",
    "\n",
    "    # joining files with concat and read_csv\n",
    "    df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "    df.sort_values('datetime', inplace=True, ascending=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    df.to_csv('./merged/cleaned.csv', index=False, sep=',')\n",
    "    \n",
    "\n",
    "def main():\n",
    "    merge()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb38ce",
   "metadata": {
    "id": "12bb38ce"
   },
   "source": [
    "## - Concat two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2dc052",
   "metadata": {
    "id": "9d2dc052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48472, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv('raw_scraped/2021-12.csv')\n",
    "data2 = pd.read_csv('raw_scraped/2022-01.csv')\n",
    "\n",
    "concat_data = pd.concat([data1,data2])\n",
    "concat_data.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3f5e3eef",
    "b0733939",
    "10ddf5f3",
    "8990d29c",
    "5086c6c0",
    "ec8f41d9",
    "12bb38ce",
    "8351db08",
    "ce292406",
    "94862698",
    "BjVW0mYBEMMH",
    "07dae0b5",
    "89bcca8d",
    "da62310e",
    "C45M-Rribw5C",
    "8149c9fb",
    "019da1ef",
    "d0baccbd",
    "9e9b5b97",
    "f0dde321",
    "_nADI-wV9VAS",
    "qcT4gSRf8GDf",
    "22294383",
    "QiQ2PJVj8qRJ",
    "h3PdYjVs-k2u"
   ],
   "name": "BSblocker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
