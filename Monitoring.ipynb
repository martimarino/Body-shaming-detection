{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb75898",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afefbb",
   "metadata": {
    "id": "86afefbb"
   },
   "source": [
    "# Online monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17116931",
   "metadata": {
    "id": "17116931"
   },
   "source": [
    "## - Extract peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6461cf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4416,
     "status": "ok",
     "timestamp": 1657795609250,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "d6461cf5",
    "outputId": "8ecc070d-a96a-4985-c11f-6be34397d63b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# 2022-03-10\n",
    "# 2022-04-09\n",
    "# 2022-06-18\n",
    "\n",
    "def extract_peaks():\n",
    "\n",
    "    df = pd.read_csv('./cleaned/2022-03-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "    end_peak = '2022-03-10 23:59:59+00:00'\n",
    "    start_peak = '2022-03-10 00:00:00+00:00'\n",
    "    start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "    peak.to_csv('./monitoring/period1.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "    df = pd.read_csv('./cleaned/2022-04-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "    end_peak = '2022-04-09 23:59:59+00:00'\n",
    "    start_peak = '2022-04-09 00:00:00+00:00'\n",
    "    start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "    peak.to_csv('./monitoring/period2.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "    df = pd.read_csv('./cleaned/2022-06-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "    end_peak = '2022-06-18 23:59:59+00:00'\n",
    "    start_peak = '2022-06-18 00:00:00+00:00'\n",
    "    start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "    peak.to_csv('./monitoring/period4.csv', index=False, sep=',')\n",
    "\n",
    "    print(\"Peaks extracted \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79f72a",
   "metadata": {},
   "source": [
    "## - Extract labelled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0b6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_labeled(in_file, out_file):\n",
    "\n",
    "    df = pd.read_csv(in_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "    tweets_list = []\n",
    "    print(df.shape)\n",
    "\n",
    "    labeled = df[(df.target == '0') | (df.target == '1')]\n",
    "    labeled.to_csv(out_file, index=False, sep=',')\n",
    "\n",
    "    return (labeled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68b40d",
   "metadata": {
    "id": "5e68b40d"
   },
   "source": [
    "# Concept drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d860c7",
   "metadata": {},
   "source": [
    "## - Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311e7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "\n",
    "def cd_training(path, data, i, c):\n",
    "\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    tweets = data.text\n",
    "    targets = data.target\n",
    "\n",
    "    model = {'name': 'ComplementNB', 'fun': ComplementNB()}\n",
    "\n",
    "    # model building\n",
    "    model['pipeline'] = Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "                                ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True)),\n",
    "                                ('fselect', SelectPercentile(chi2, percentile=85)),\n",
    "                                #('fselect', SelectKBest(chi2, k='all')),      \n",
    "                                ('clf', model['fun'])])\n",
    "\n",
    "    m = model['pipeline'].fit(tweets, targets)\n",
    "    \n",
    "    print(\"Number of features: \", len(model['pipeline']['vect'].vocabulary_))\n",
    "    \n",
    "    # save model\n",
    "    if c =='i':\n",
    "        filename = model['name']+'_interval'+str(i)+'.sav'\n",
    "    elif c == 's':\n",
    "        filename = model['name']+'_slide'+str(i)+'.sav'\n",
    "    \n",
    "    pickle.dump(m, open(path+'/'+filename, 'wb'))\n",
    "\n",
    "    print(\"\\nModel correctly saved!\\n\")\n",
    "    print('â”€' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf335ec",
   "metadata": {},
   "source": [
    "## - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2e3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_test(path, data, i, c):\n",
    "    \n",
    "    print(\"Testing...\")\n",
    "\n",
    "    tweets = data['text']\n",
    "    targets = data['target']\n",
    "\n",
    "    \n",
    "    if c == 'st':\n",
    "        loaded_model = pickle.load(open('models_result/85/models_85/'+model['name']+'.sav', 'rb'))\n",
    "    elif c == 'i':\n",
    "        loaded_model = pickle.load(open(path+model['name']+'_interval'+str(i)+'.sav', 'rb'))\n",
    "    elif c == 's':\n",
    "        loaded_model = pickle.load(open(path+model['name']+'_slide'+str(i)+'.sav', 'rb'))\n",
    "    \n",
    "    score = loaded_model.score(tweets, targets)\n",
    "    print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "    y_predict = loaded_model.predict(tweets)\n",
    "\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                          target_names=['0', '1'])\n",
    "    print(rep, '\\n')\n",
    "\n",
    "    # save reports\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                target_names=['0', '1'], output_dict=True)\n",
    "    df = pd.DataFrame(rep).transpose()\n",
    "    \n",
    "    if c == 'st':\n",
    "        df.to_csv(path+'period'+str(i)+'-report.csv')\n",
    "    if c == 'i':\n",
    "        df.to_csv(path+'interval'+str(i)+'-report.csv')\n",
    "    elif c == 's':\n",
    "        df.to_csv(path+'slide'+str(i)+'-report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31479da",
   "metadata": {},
   "source": [
    "## - Create window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119c5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(path, i, file1, file2, c):\n",
    "\n",
    "    print(\"Creating window...\")\n",
    "    df1 = pd.read_csv(file1, index_col=False, delimiter=\",\")\n",
    "    df1.sort_values('datetime', inplace=True, ascending=True)\n",
    "    print(\"Previous window: \", df1.shape)\n",
    "    \n",
    "    if c == 's':\n",
    "        if i == 1:\n",
    "            data1 = df1.tail(-176)\n",
    "        else:\n",
    "            data1 = df1.tail(-80)\n",
    "        print(\"Tweets deleted: \", data1.shape)\n",
    "        \n",
    "    data2 = pd.read_csv(file2, index_col=False, delimiter=\",\")\n",
    "    data2.sort_values('datetime', inplace=True, ascending=True)\n",
    "    print(\"New tweets: \", data2.shape)\n",
    "    \n",
    "    if c == 'i':\n",
    "        window = pd.concat([df1,data2])\n",
    "    elif c == 's':\n",
    "        window = pd.concat([data1,data2])\n",
    "        \n",
    "    print(\"New window: \", window.shape)\n",
    "    \n",
    "    if c == 'i':\n",
    "        window_name = path+'interval'+str(i)+'.csv'\n",
    "    elif c == 's':\n",
    "        window_name = path+'slide'+str(i)+'.csv'\n",
    "    window.to_csv(window_name, index=False)\n",
    "    \n",
    "    return window_name, window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85d8e1",
   "metadata": {},
   "source": [
    "\n",
    "## - Get files for building a new window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5009e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path, i, c):\n",
    "    \n",
    "    list = []\n",
    "    j = i-1\n",
    "    if i == 1:\n",
    "        list.append('monitoring/12-01-rebalanced-only-labeled.csv')\n",
    "        list.append('monitoring/2022-02-labeled-only.csv')\n",
    "        \n",
    "    else: # 2 3 4 5\n",
    "        if c == 'i':\n",
    "            list.append(path+'interval'+str(j)+'.csv')\n",
    "        elif c == 's':\n",
    "            list.append(path+'slide'+str(j)+'.csv')\n",
    "        list.append('monitoring/period-'+str(j)+'-labeled-only.csv')\n",
    "    \n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86920439",
   "metadata": {
    "id": "86920439"
   },
   "source": [
    "## - Static model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e2fe2eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6415,
     "status": "ok",
     "timestamp": 1657795812698,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "6e2fe2eb",
    "outputId": "bc4a176d-fd99-46c0-e82c-9557710bd712",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "def static_cd(model):\n",
    "    \n",
    "    static_path = 'monitoring/concept_drift/static/'\n",
    "        \n",
    "    for i in range(1,5):\n",
    "        \n",
    "        test_set_file = './monitoring/period-'+str(i)+'-labeled-only.csv'\n",
    "        test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "        test_data = preprocess(test_data)\n",
    "        test_data = elaborate(test_data)\n",
    "\n",
    "        cd_test(static_path, test_data, i, 'st')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df5686",
   "metadata": {
    "id": "f8df5686"
   },
   "source": [
    "## - Sliding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "518db23f",
   "metadata": {
    "id": "518db23f"
   },
   "outputs": [],
   "source": [
    "def sliding_cd(model):       \n",
    "    \n",
    "    sliding_path = 'monitoring/concept_drift/sliding/'\n",
    "\n",
    "    for i in range(1,6):\n",
    "        \n",
    "        \n",
    "        print(\"\\n*********** SLIDING MODEL ************\\n\")\n",
    "        \n",
    "        #create window\n",
    "        list = get_files(sliding_path , i, 's')  # old and new tweets\n",
    "        print(\"Files to merge: \", list)\n",
    "        file1 = list[0]\n",
    "        file2 = list[1]\n",
    "        \n",
    "        slide_name, slide = create_window(sliding_path, i, file1, file2, 's')\n",
    "        print(\"File created: \", slide_name, \"\\n\")\n",
    "\n",
    "        #train\n",
    "        training_data = preprocess(slide)\n",
    "        training_data = elaborate(training_data)\n",
    "        \n",
    "        cd_training(sliding_path, training_data, i, 's')\n",
    "        \n",
    "        #test on next month\n",
    "        \n",
    "        if i < 5:\n",
    "        \n",
    "            test_set_file = './labeled/period-'+str(i)+'-labeled-only.csv'\n",
    "            test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "            test_data = preprocess(test_data)\n",
    "            test_data = elaborate(test_data)\n",
    "\n",
    "            cd_test(sliding_path, test_data, i, 's') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7d5c5",
   "metadata": {
    "id": "17e7d5c5"
   },
   "source": [
    "## - Incremental model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4febfb93",
   "metadata": {
    "id": "4febfb93",
    "outputId": "b20c545f-01f0-4af6-c217-08e90b992d9e"
   },
   "outputs": [],
   "source": [
    "def incremental_cd(model):\n",
    "    \n",
    "    incremental_path = 'monitoring/concept_drift/incremental/'\n",
    "\n",
    "    for i in range(1,6):\n",
    "        \n",
    "        print(\"\\n*********** INCREMENTAL MODEL ************\\n\")\n",
    "        \n",
    "        #create window\n",
    "        list = get_files(incremental_path, i, 'i')  # old and new tweets\n",
    "        print(\"Files to merge: \", list)\n",
    "        file1 = list[0]\n",
    "        file2 = list[1]\n",
    "        \n",
    "        interval_name, interval = create_window(incremental_path, i, file1, file2, 'i')\n",
    "        print(\"File created: \", interval_name, \"\\n\")\n",
    "        \n",
    "        #train\n",
    "        training_data = preprocess(interval)\n",
    "        training_data = elaborate(training_data)\n",
    "        \n",
    "        cd_training(incremental_path, training_data, i, 'i') \n",
    "        \n",
    "        #test on next month\n",
    "        \n",
    "        if i < 5:\n",
    "                    \n",
    "            test_set_file = './labeled/period-'+str(i)+'-labeled-only.csv'\n",
    "            test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "            test_data = preprocess(test_data)\n",
    "            test_data = elaborate(test_data)\n",
    "\n",
    "            cd_test(incremental_path, test_data, i, 'i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcff96",
   "metadata": {},
   "source": [
    "## - Concept drift main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0affc392",
   "metadata": {
    "id": "0affc392"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing done\n",
      "Elaboration done\n",
      "\n",
      "\n",
      "Testing...\n",
      "Test score: 73.75 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73        40\n",
      "           1       0.73      0.75      0.74        40\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.74      0.74      0.74        80\n",
      "weighted avg       0.74      0.74      0.74        80\n",
      " \n",
      "\n",
      "Preprocessing done\n",
      "Elaboration done\n",
      "\n",
      "\n",
      "Testing...\n",
      "Test score: 73.75 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75        40\n",
      "           1       0.76      0.70      0.73        40\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.74      0.74      0.74        80\n",
      "weighted avg       0.74      0.74      0.74        80\n",
      " \n",
      "\n",
      "Preprocessing done\n",
      "Elaboration done\n",
      "\n",
      "\n",
      "Testing...\n",
      "Test score: 71.25 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71        40\n",
      "           1       0.71      0.72      0.72        40\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.71      0.71      0.71        80\n",
      "weighted avg       0.71      0.71      0.71        80\n",
      " \n",
      "\n",
      "Preprocessing done\n",
      "Elaboration done\n",
      "\n",
      "\n",
      "Testing...\n",
      "Test score: 65.00 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.72      0.67        40\n",
      "           1       0.68      0.57      0.62        40\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.65      0.65      0.65        80\n",
      "weighted avg       0.65      0.65      0.65        80\n",
      " \n",
      "\n",
      "\n",
      "*********** INCREMENTAL MODEL ************\n",
      "\n",
      "Files to merge:  ['monitoring/12-01-rebalanced-only-labeled.csv', 'monitoring/2022-02-labeled-only.csv']\n",
      "Creating window...\n",
      "Previous window:  (1576, 4)\n",
      "New tweets:  (176, 4)\n",
      "New window:  (1752, 4)\n",
      "File created:  monitoring/concept_drift/incremental/interval1.csv \n",
      "\n",
      "Preprocessing done\n",
      "Elaboration done\n",
      "\n",
      "\n",
      "Training...\n",
      "Number of features:  4734\n",
      "\n",
      "Model correctly saved!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Preprocessing done\n",
      "Elaboration done\n",
      "\n",
      "\n",
      "Testing...\n",
      "Test score: 73.75 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75        40\n",
      "           1       0.76      0.70      0.73        40\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.74      0.74      0.74        80\n",
      "weighted avg       0.74      0.74      0.74        80\n",
      " \n",
      "\n",
      "\n",
      "*********** INCREMENTAL MODEL ************\n",
      "\n",
      "Files to merge:  ['monitoring/concept_drift/incremental/interval1.csv', 'monitoring/period-1-labeled-only.csv']\n",
      "Creating window...\n",
      "Previous window:  (1752, 4)\n",
      "New tweets:  (80, 4)\n",
      "New window:  (1832, 4)\n",
      "File created:  monitoring/concept_drift/incremental/interval2.csv \n",
      "\n",
      "Preprocessing done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplementNB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m\"\u001b[39m: ComplementNB()}\n\u001b[0;32m     17\u001b[0m static_cd(model)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mincremental_cd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m sliding_cd(model)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mincremental_cd\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#train\u001b[39;00m\n\u001b[0;32m     19\u001b[0m training_data \u001b[38;5;241m=\u001b[39m preprocess(interval)\n\u001b[1;32m---> 20\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43melaborate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m cd_training(incremental_path, training_data, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#test on next month\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19592\\3570277908.py:40\u001b[0m, in \u001b[0;36melaborate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     35\u001b[0m elaborated \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, tweet \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# print(tweet['text'])\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     new_tweet \u001b[38;5;241m=\u001b[39m \u001b[43melaborating_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     41\u001b[0m     elaborated\u001b[38;5;241m.\u001b[39mappend(new_tweet)\n\u001b[0;32m     42\u001b[0m     data\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m new_tweet\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19592\\3570277908.py:16\u001b[0m, in \u001b[0;36melaborating_steps\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m     14\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(t)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# print(tokens)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mremove_stopwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# print(tokens)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m stemmed_words \u001b[38;5;241m=\u001b[39m stem(tokens)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19592\\1911209648.py:8\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(tokens):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# nltk.download('stopwords')\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mitalian\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:334\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    333\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py:41\u001b[0m, in \u001b[0;36mpy3_data.<locals>._decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     40\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args[\u001b[38;5;241m0\u001b[39m], add_py3_data(args[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m args[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m init_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py:311\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    model = {\"name\": \"ComplementNB\", \"fun\": ComplementNB()}\n",
    "    \n",
    "    static_cd(model)\n",
    "    \n",
    "    incremental_cd(model)\n",
    "    \n",
    "    sliding_cd(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87c8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d6a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3f5e3eef",
    "b0733939",
    "10ddf5f3",
    "8990d29c",
    "5086c6c0",
    "ec8f41d9",
    "12bb38ce",
    "8351db08",
    "ce292406",
    "94862698",
    "BjVW0mYBEMMH",
    "07dae0b5",
    "89bcca8d",
    "da62310e",
    "C45M-Rribw5C",
    "8149c9fb",
    "019da1ef",
    "d0baccbd",
    "9e9b5b97",
    "f0dde321",
    "_nADI-wV9VAS",
    "qcT4gSRf8GDf",
    "22294383",
    "QiQ2PJVj8qRJ",
    "h3PdYjVs-k2u"
   ],
   "name": "BSblocker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
