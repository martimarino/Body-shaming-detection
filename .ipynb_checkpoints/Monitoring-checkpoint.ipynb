{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0eb75898",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86afefbb",
   "metadata": {
    "id": "86afefbb"
   },
   "source": [
    "# Online monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17116931",
   "metadata": {
    "id": "17116931"
   },
   "source": [
    "## - Extract peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d6461cf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4416,
     "status": "ok",
     "timestamp": 1657795609250,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "d6461cf5",
    "outputId": "8ecc070d-a96a-4985-c11f-6be34397d63b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# 2022-03-10\n",
    "# 2022-04-09\n",
    "# 2022-06-18\n",
    "\n",
    "def extract_peaks():\n",
    "\n",
    "    df = pd.read_csv('./cleaned/2022-03-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "    end_peak = '2022-03-10 23:59:59+00:00'\n",
    "    start_peak = '2022-03-10 00:00:00+00:00'\n",
    "    start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "    peak.to_csv('./monitoring/period1.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "    df = pd.read_csv('./cleaned/2022-04-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "    end_peak = '2022-04-09 23:59:59+00:00'\n",
    "    start_peak = '2022-04-09 00:00:00+00:00'\n",
    "    start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "    peak.to_csv('./monitoring/period2.csv', index=False, sep=',')\n",
    "\n",
    "\n",
    "    df = pd.read_csv('./cleaned/2022-06-cleaned.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "    end_peak = '2022-06-18 23:59:59+00:00'\n",
    "    start_peak = '2022-06-18 00:00:00+00:00'\n",
    "    start = time.strptime(start_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    end = time.strptime(end_peak, \"%Y-%m-%d %H:%M:%S+00:00\")\n",
    "    peak = df[(df['datetime'] > start_peak) & (df['datetime'] < end_peak)]\n",
    "\n",
    "    peak.to_csv('./monitoring/period4.csv', index=False, sep=',')\n",
    "\n",
    "    print(\"Peaks extracted \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c79f72a",
   "metadata": {},
   "source": [
    "## - Extract labelled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6a0b6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_labeled(in_file, out_file):\n",
    "\n",
    "    df = pd.read_csv(in_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "    tweets_list = []\n",
    "    print(df.shape)\n",
    "\n",
    "    labeled = df[(df.target == '0') | (df.target == '1')]\n",
    "    labeled.to_csv(out_file, index=False, sep=',')\n",
    "\n",
    "    return (labeled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e68b40d",
   "metadata": {
    "id": "5e68b40d"
   },
   "source": [
    "# Concept drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d860c7",
   "metadata": {},
   "source": [
    "## - Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "311e7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "\n",
    "def cd_training(path, data, i, c):\n",
    "\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    tweets = data.text\n",
    "    targets = data.target\n",
    "\n",
    "    model = {'name': 'ComplementNB', 'fun': ComplementNB()}\n",
    "\n",
    "    # model building\n",
    "    model['pipeline'] = Pipeline(steps=[('vect', CountVectorizer(ngram_range=(1, 1))),\n",
    "                                ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True)),\n",
    "                                ('fselect', SelectPercentile(chi2, percentile=85)),\n",
    "                                #('fselect', SelectKBest(chi2, k='all')),      \n",
    "                                ('clf', model['fun'])])\n",
    "\n",
    "    m = model['pipeline'].fit(tweets, targets)\n",
    "    \n",
    "    print(\"Number of features: \", len(model['pipeline']['vect'].vocabulary_))\n",
    "    \n",
    "    # save model\n",
    "    if c =='i':\n",
    "        filename = model['name']+'_interval'+str(i)+'.sav'\n",
    "    elif c == 's':\n",
    "        filename = model['name']+'_slide'+str(i)+'.sav'\n",
    "    \n",
    "    pickle.dump(m, open(path+'/'+filename, 'wb'))\n",
    "\n",
    "    print(\"\\nModel correctly saved!\\n\")\n",
    "    print('â”€' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf335ec",
   "metadata": {},
   "source": [
    "## - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2e2e3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_test(path, data, i, c):\n",
    "    \n",
    "    print(\"Testing...\")\n",
    "\n",
    "    tweets = data['text']\n",
    "    targets = data['target']\n",
    "\n",
    "    \n",
    "    if c == 'st':\n",
    "        loaded_model = pickle.load(open('models_result/85/models_85/'+model['name']+'.sav', 'rb'))\n",
    "    elif c == 'i':\n",
    "        loaded_model = pickle.load(open(path+model['name']+'_interval'+str(i)+'.sav', 'rb'))\n",
    "    elif c == 's':\n",
    "        loaded_model = pickle.load(open(path+model['name']+'_slide'+str(i)+'.sav', 'rb'))\n",
    "    \n",
    "    score = loaded_model.score(tweets, targets)\n",
    "    print(\"Test score: {0:.2f} %\".format(100 * score))\n",
    "    y_predict = loaded_model.predict(tweets)\n",
    "\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                          target_names=['0', '1'])\n",
    "    print(rep, '\\n')\n",
    "\n",
    "    # save reports\n",
    "    rep = classification_report(targets, y_predict,\n",
    "                                target_names=['0', '1'], output_dict=True)\n",
    "    df = pd.DataFrame(rep).transpose()\n",
    "    \n",
    "    if c == 'st':\n",
    "        df.to_csv(path+'period'+str(i)+'-report.csv')\n",
    "    if c == 'i':\n",
    "        df.to_csv(path+'interval'+str(i)+'-report.csv')\n",
    "    elif c == 's':\n",
    "        df.to_csv(path+'slide'+str(i)+'-report.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31479da",
   "metadata": {},
   "source": [
    "## - Create window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "119c5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_window(path, i, file1, file2, c):\n",
    "\n",
    "    print(\"Creating window...\")\n",
    "    df1 = pd.read_csv(file1, index_col=False, delimiter=\",\")\n",
    "    df1.sort_values('datetime', inplace=True, ascending=True)\n",
    "    print(\"Previous window: \", df1.shape)\n",
    "    \n",
    "    if c == 's':\n",
    "        if i == 1:\n",
    "            data1 = df1.tail(-176)\n",
    "        else:\n",
    "            data1 = df1.tail(-80)\n",
    "        print(\"Tweets deleted: \", data1.shape)\n",
    "        \n",
    "    data2 = pd.read_csv(file2, index_col=False, delimiter=\",\")\n",
    "    data2.sort_values('datetime', inplace=True, ascending=True)\n",
    "    print(\"New tweets: \", data2.shape)\n",
    "    \n",
    "    if c == 'i':\n",
    "        window = pd.concat([df1,data2])\n",
    "    elif c == 's':\n",
    "        window = pd.concat([data1,data2])\n",
    "        \n",
    "    print(\"New window: \", window.shape)\n",
    "    \n",
    "    if c == 'i':\n",
    "        window_name = path+'interval'+str(i)+'.csv'\n",
    "    elif c == 's':\n",
    "        window_name = path+'slide'+str(i)+'.csv'\n",
    "    window.to_csv(window_name, index=False)\n",
    "    \n",
    "    return window_name, window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85d8e1",
   "metadata": {},
   "source": [
    "\n",
    "## - Get files for building a new window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5009e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path, i, c):\n",
    "    \n",
    "    list = []\n",
    "    j = i-1\n",
    "    if i == 1:\n",
    "        list.append('monitoring/12-01-rebalanced-only-labeled.csv')\n",
    "        list.append('monitoring/2022-02-labeled-only.csv')\n",
    "        \n",
    "    else: # 2 3 4 5\n",
    "        if c == 'i':\n",
    "            list.append(path+'interval'+str(j)+'.csv')\n",
    "        elif c == 's':\n",
    "            list.append(path+'slide'+str(j)+'.csv')\n",
    "        list.append('monitoring/period-'+str(j)+'-labeled-only.csv')\n",
    "    \n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86920439",
   "metadata": {
    "id": "86920439"
   },
   "source": [
    "## - Static model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e2fe2eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6415,
     "status": "ok",
     "timestamp": 1657795812698,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "6e2fe2eb",
    "outputId": "bc4a176d-fd99-46c0-e82c-9557710bd712",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "def static_cd(model):\n",
    "    \n",
    "    static_path = 'monitoring/concept_drift/static/'\n",
    "        \n",
    "    for i in range(1,5):\n",
    "        \n",
    "        test_set_file = './monitoring/period-'+str(i)+'-labeled-only.csv'\n",
    "        test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "        test_data = preprocess(test_data)\n",
    "        test_data = elaborate(test_data)\n",
    "\n",
    "        cd_test(static_path, test_data, i, 'st')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df5686",
   "metadata": {
    "id": "f8df5686"
   },
   "source": [
    "## - Sliding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "518db23f",
   "metadata": {
    "id": "518db23f"
   },
   "outputs": [],
   "source": [
    "def sliding_cd(model):       \n",
    "    \n",
    "    sliding_path = 'monitoring/concept_drift/sliding/'\n",
    "\n",
    "    for i in range(1,6):\n",
    "        \n",
    "        \n",
    "        print(\"\\n*********** SLIDING MODEL ************\\n\")\n",
    "        \n",
    "        #create window\n",
    "        list = get_files(sliding_path , i, 's')  # old and new tweets\n",
    "        print(\"Files to merge: \", list)\n",
    "        file1 = list[0]\n",
    "        file2 = list[1]\n",
    "        \n",
    "        slide_name, slide = create_window(sliding_path, i, file1, file2, 's')\n",
    "        print(\"File created: \", slide_name, \"\\n\")\n",
    "\n",
    "        #train\n",
    "        training_data = preprocess(slide)\n",
    "        training_data = elaborate(training_data)\n",
    "        \n",
    "        cd_training(sliding_path, training_data, i, 's')\n",
    "        \n",
    "        #test on next month\n",
    "        \n",
    "        if i < 5:\n",
    "        \n",
    "            test_set_file = './labeled/period-'+str(i)+'-labeled-only.csv'\n",
    "            test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "            test_data = preprocess(test_data)\n",
    "            test_data = elaborate(test_data)\n",
    "\n",
    "            cd_test(sliding_path, test_data, i, 's') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7d5c5",
   "metadata": {
    "id": "17e7d5c5"
   },
   "source": [
    "## - Incremental model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4febfb93",
   "metadata": {
    "id": "4febfb93",
    "outputId": "b20c545f-01f0-4af6-c217-08e90b992d9e"
   },
   "outputs": [],
   "source": [
    "def incremental_cd(model):\n",
    "    \n",
    "    incremental_path = 'monitoring/concept_drift/incremental/'\n",
    "\n",
    "    for i in range(1,6):\n",
    "        \n",
    "        print(\"\\n*********** INCREMENTAL MODEL ************\\n\")\n",
    "        \n",
    "        #create window\n",
    "        list = get_files(incremental_path, i, 'i')  # old and new tweets\n",
    "        print(\"Files to merge: \", list)\n",
    "        file1 = list[0]\n",
    "        file2 = list[1]\n",
    "        \n",
    "        interval_name, interval = create_window(incremental_path, i, file1, file2, 'i')\n",
    "        print(\"File created: \", interval_name, \"\\n\")\n",
    "        \n",
    "        #train\n",
    "        training_data = preprocess(interval)\n",
    "        training_data = elaborate(training_data)\n",
    "        \n",
    "        cd_training(incremental_path, training_data, i, 'i') \n",
    "        \n",
    "        #test on next month\n",
    "        \n",
    "        if i < 5:\n",
    "                    \n",
    "            test_set_file = './labeled/period-'+str(i)+'-labeled-only.csv'\n",
    "            test_data = pd.read_csv(test_set_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "            test_data = preprocess(test_data)\n",
    "            test_data = elaborate(test_data)\n",
    "\n",
    "            cd_test(incremental_path, test_data, i, 'i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcff96",
   "metadata": {},
   "source": [
    "## - Concept drift main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0affc392",
   "metadata": {
    "id": "0affc392"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ComplementNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m     model \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplementNB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mComplementNB\u001b[49m()}\n\u001b[0;32m      5\u001b[0m     static_cd(model)\n\u001b[0;32m      7\u001b[0m     incremental_cd(model)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ComplementNB' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_selection import chi2, SelectPercentile\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    model = {\"name\": \"ComplementNB\", \"fun\": ComplementNB()}\n",
    "    \n",
    "    static_cd(model)\n",
    "    \n",
    "    incremental_cd(model)\n",
    "    \n",
    "    sliding_cd(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87c8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d6a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3f5e3eef",
    "b0733939",
    "10ddf5f3",
    "8990d29c",
    "5086c6c0",
    "ec8f41d9",
    "12bb38ce",
    "8351db08",
    "ce292406",
    "94862698",
    "BjVW0mYBEMMH",
    "07dae0b5",
    "89bcca8d",
    "da62310e",
    "C45M-Rribw5C",
    "8149c9fb",
    "019da1ef",
    "d0baccbd",
    "9e9b5b97",
    "f0dde321",
    "_nADI-wV9VAS",
    "qcT4gSRf8GDf",
    "22294383",
    "QiQ2PJVj8qRJ",
    "h3PdYjVs-k2u"
   ],
   "name": "BSblocker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
