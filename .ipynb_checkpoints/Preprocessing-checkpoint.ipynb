{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6166d464",
   "metadata": {},
   "source": [
    "# Clean data before labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd39e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean():\n",
    "\n",
    "    input_file = './raw_scraped/2022-07.csv'\n",
    "    output_file = './cleaned/2022-07-cleaned.csv'\n",
    "\n",
    "    # remove dup\n",
    "    df = pd.read_csv(input_file, index_col=False, delimiter=\",\")\n",
    "    df.drop_duplicates(subset=['text', 'username'])\n",
    "\n",
    "    dfk = pd.read_csv(\"./keys.txt\", sep=';')\n",
    "    Words = dfk['Words'].values\n",
    "\n",
    "    tweets_list = []\n",
    "    how_many = 0\n",
    "    found = 0\n",
    "\n",
    "\n",
    "    # insert 'target' column\n",
    "    df.insert(3, 'target', \" \", allow_duplicates=True)\n",
    "    df.to_csv(output_file, index=False, sep=',')\n",
    "\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        # remove URLs and mentions\n",
    "        df.at[i, 'text'] = re.sub(r\"(?:\\@|https?\\://)\\S+\", '', df.at[i, 'text'], flags=re.MULTILINE)\n",
    "\n",
    "        # remove new lines\n",
    "        if df.at[i, 'text'].endswith(\"\\n\") or df.at[i, 'text'].endswith(\"\\r\"):\n",
    "            df.at[i, 'text'] = df.at[i, 'text'].replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "        else:\n",
    "            df.at[i, 'text'] = df.at[i, 'text'].replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "        # remove multiple spaces\n",
    "        df.at[i, 'text'] = re.sub('\\\\s+', ' ', df.at[i, 'text'])\n",
    "\n",
    "        # remove tweets without keywords\n",
    "        for word in Words:\n",
    "\n",
    "            found = 0\n",
    "\n",
    "            if word in df.at[i, 'text'].lower():\n",
    "\n",
    "                found = 1\n",
    "\n",
    "                tweets_list.append(\n",
    "                    [df.at[i, 'datetime'], df.at[i, 'text'], df.at[i, 'username'], df.at[i, 'target']])\n",
    "\n",
    "                how_many = how_many + 1\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Tweets filtered: \", how_many, \"\\t\\tdate: \", df.at[i, 'datetime'])\n",
    "\n",
    "                break\n",
    "        if found == 0:\n",
    "            print(\"Deleted: \", df.at[i, 'text'])\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['datetime', 'text', 'username', 'target'])\n",
    "    tweets_df.to_csv(output_file, index=False, sep=',')\n",
    "    print(tweets_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42020565",
   "metadata": {},
   "source": [
    "## - Remove tweets with few occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def remove_few_occurences():\n",
    "    \n",
    "    data = pd.read_csv(\"./cleaned/2021-12-cleaned.csv\")\n",
    "    white_list = [\"pelato\", \"pelata\", \"nano\", \"nana\", \"obeso\", \"obesa\", \"cozza\",\n",
    "                 \"ciccione\", \"grasso\", \"grassa\"]\n",
    "    black_list = [\"boiler\", \"anoressica\", \"anoressico\", \"cicciona\", \"nasone\", \n",
    "                 \"racchia\", \"culona\", \"obesa\"]\n",
    "\n",
    "    tweets_list = []\n",
    "    black = False \n",
    "    white = False\n",
    "    deleted = 0\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        if any(word in data.at[i, 'Text'].lower() for word in black_list):\n",
    "            black = True\n",
    "        if any(word in data.at[i, 'Text'].lower() for word in white_list):\n",
    "            white = True\n",
    "\n",
    "        if black and not white:\n",
    "            deleted+=1\n",
    "            #print(deleted)\n",
    "            #print(data.at[i, 'Text'])\n",
    "            continue\n",
    "        else:\n",
    "            tweets_list.append(\n",
    "                [data.at[i, 'Datetime'], data.at[i, 'Text'], \n",
    "                 data.at[i, 'Username']])   \n",
    "\n",
    "            #tweets_df.to_csv(\"./labeled/prova-2.csv\", index=False, sep=',')\n",
    "        black = False \n",
    "        white = False\n",
    "\n",
    "    print(tweets_df.shape)\n",
    "    print(deleted)\n",
    "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Text', 'Username'])\n",
    "    tweets_df.to_csv(\"./cleaned/2021-12-filtered.csv\", index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8b185",
   "metadata": {},
   "source": [
    "# Extract labelled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_labeled(in_file, out_file):\n",
    "\n",
    "    df = pd.read_csv(in_file, index_col=False, delimiter=\",\")\n",
    "\n",
    "    tweets_list = []\n",
    "    print(df.shape)\n",
    "\n",
    "    labeled = df[(df.target == '0') | (df.target == '1')]\n",
    "    labeled.to_csv(out_file, index=False, sep=',')\n",
    "\n",
    "    return (labeled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c5d19c",
   "metadata": {},
   "source": [
    "## - Verify balanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c6b9c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def verify_balanced():\n",
    "    data = pd.read_csv('./labeled/12-01-rebalanced-only-labeled.csv', index_col=False, delimiter=\",\")\n",
    "\n",
    "    tot0 = data.query(\"target == 0\").shape[0]\n",
    "    tot1 = data.query(\"target == 1\").shape[0]\n",
    "\n",
    "    print(\"Tot 0 --> \", tot0)\n",
    "    print(\"Tot 1 --> \", tot1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019da1ef",
   "metadata": {
    "id": "019da1ef"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0baccbd",
   "metadata": {
    "id": "d0baccbd"
   },
   "source": [
    "## - Remove punctuation marks, brackets, quotes, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fb518",
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1657314641174,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "f77fb518"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join([i for i in str(text) if i not in string.punctuation])\n",
    "    text = text.replace('\\u201D', \" \")\n",
    "    text = text.replace('\\u2018', \" \")\n",
    "    text = text.replace('\\u2019', \" \")\n",
    "    text = text.replace('\\u201c', \" \")\n",
    "    text = text.replace('\\u2026', \" \")\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9b5b97",
   "metadata": {
    "id": "9e9b5b97"
   },
   "source": [
    "## - Text reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f109a",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1657314641415,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "1a8f109a"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def text_reformat(text):\n",
    "    # remove two or more dots\n",
    "    text = re.sub(r'\\.{2,}', ' ', text)\n",
    "    # remove two or more letters: { bellooooo -> bello}\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dde321",
   "metadata": {
    "id": "f0dde321"
   },
   "source": [
    "## - Remove emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc6e650",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657314641416,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "bfc6e650"
   },
   "outputs": [],
   "source": [
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "                      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                      u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U00002702-\\U000027B0\"\n",
    "                      u\"\\U000024C2-\\U0001F251\"\n",
    "                      u\"\\U0001f926-\\U0001f937\"\n",
    "                      u\"\\U00010000-\\U0010ffff\"\n",
    "                      u\"\\u2640-\\u2642\"\n",
    "                      u\"\\u2600-\\u2B55\"\n",
    "                      u\"\\u200d\"\n",
    "                      u\"\\u23cf\"\n",
    "                      u\"\\u23e9\"\n",
    "                      u\"\\u231a\"\n",
    "                      u\"\\ufe0f\"  # dingbats\n",
    "                      u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_nADI-wV9VAS",
   "metadata": {
    "id": "_nADI-wV9VAS"
   },
   "source": [
    "## - Preprocessing call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XKK4Qc1p6ljt",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657314641416,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "XKK4Qc1p6ljt"
   },
   "outputs": [],
   "source": [
    "def preprocessing_steps(data):\n",
    "    new_data = remove_punctuation(data)\n",
    "    new_data = text_reformat(new_data)\n",
    "    new_data = remove_emojis(new_data)\n",
    "\n",
    "    return new_data\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "\n",
    "    array = []\n",
    "\n",
    "    for index, tweet in data.iterrows():\n",
    "\n",
    "        # print(tweet['Text'])\n",
    "        new_data = preprocessing_steps(tweet['text'])\n",
    "        array.append(new_data)\n",
    "        # print(new_data, \"\\n\")\n",
    "\n",
    "    data['text'] = array\n",
    "\n",
    "    print(\"Preprocessing done\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qcT4gSRf8GDf",
   "metadata": {
    "id": "qcT4gSRf8GDf"
   },
   "source": [
    "# Elaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3659ab",
   "metadata": {
    "id": "8f3659ab"
   },
   "source": [
    "## - Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f888169",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1657314643600,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "1f888169"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('italian'))\n",
    "    \n",
    "    return [i for i in tokens if i not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22294383",
   "metadata": {
    "id": "22294383"
   },
   "source": [
    "## - Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b8335",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657314643838,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "980b8335"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stem(tokens):\n",
    "    # the stemmer requires a language parameter\n",
    "    snow_stemmer = SnowballStemmer(language='italian')\n",
    "\n",
    "    return [snow_stemmer.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QiQ2PJVj8qRJ",
   "metadata": {
    "id": "QiQ2PJVj8qRJ"
   },
   "source": [
    "## - Remove miningless words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v2QXZrIQ865b",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1657314643838,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "v2QXZrIQ865b"
   },
   "outputs": [],
   "source": [
    "def miningfull_words(stemmed):\n",
    "    return [word for word in stemmed if len(word) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h3PdYjVs-k2u",
   "metadata": {
    "id": "h3PdYjVs-k2u"
   },
   "source": [
    "## - Remove features with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tx9QrjEz-pav",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1657314643838,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "tx9QrjEz-pav"
   },
   "outputs": [],
   "source": [
    "def remove_numbers(mean_words):\n",
    "    return [word for word in mean_words if not word.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0Y4noSc9REE",
   "metadata": {
    "id": "d0Y4noSc9REE"
   },
   "source": [
    "## - Elaboration call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dX6LQ2t9FXK",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657314643839,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "3dX6LQ2t9FXK"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def textual(text):\n",
    "    tweets = \"\"\n",
    "    for word in text:\n",
    "        tweets += word + \" \"\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def elaborating_steps(t):\n",
    "    # print(t)\n",
    "\n",
    "    tokens = word_tokenize(t)\n",
    "    # print(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    # print(tokens)\n",
    "    stemmed_words = stem(tokens)\n",
    "    # print(stemmed_words)\n",
    "    mean_words = miningfull_words(stemmed_words)\n",
    "    # print(mean_words)\n",
    "    numbers_removed = remove_numbers(mean_words)\n",
    "    # print(numbers_removed)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    elaborated_tweet = \"\"\n",
    "    for word in numbers_removed:\n",
    "        elaborated_tweet += word + \" \"\n",
    "\n",
    "    return elaborated_tweet\n",
    "\n",
    "\n",
    "def elaborate(data):\n",
    "\n",
    "    elaborated = []\n",
    "\n",
    "    for index, tweet in data.iterrows():\n",
    "\n",
    "        # print(tweet['text'])\n",
    "        new_tweet = elaborating_steps(tweet['text']).strip()\n",
    "        elaborated.append(new_tweet)\n",
    "        data.at[index, 'text'] = new_tweet\n",
    "\n",
    "    # print(elaborated)\n",
    "    data = data.sort_values(by='datetime')\n",
    "\n",
    "    print(\"Elaboration done\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3f5e3eef",
    "b0733939",
    "10ddf5f3",
    "8990d29c",
    "5086c6c0",
    "ec8f41d9",
    "12bb38ce",
    "8351db08",
    "ce292406",
    "94862698",
    "BjVW0mYBEMMH",
    "07dae0b5",
    "89bcca8d",
    "da62310e",
    "C45M-Rribw5C",
    "8149c9fb",
    "019da1ef",
    "d0baccbd",
    "9e9b5b97",
    "f0dde321",
    "_nADI-wV9VAS",
    "qcT4gSRf8GDf",
    "22294383",
    "QiQ2PJVj8qRJ",
    "h3PdYjVs-k2u"
   ],
   "name": "BSblocker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
