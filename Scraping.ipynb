{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c91c1e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22948,
     "status": "ok",
     "timestamp": 1657314632499,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "1c91c1e7",
    "outputId": "babc71ee-06e9-4e31-ef79-7c8356e1e96b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec23085",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1657314669610,
     "user": {
      "displayName": "MARTINA MARINO",
      "userId": "06117242016215180196"
     },
     "user_tz": -120
    },
    "id": "6ec23085",
    "outputId": "bd891756-f14c-4bf2-f9d3-ece2caaebf82"
   },
   "outputs": [],
   "source": [
    "# % cd '/content/drive/MyDrive/Data Mining/BSblocker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5e3eef",
   "metadata": {
    "id": "3f5e3eef"
   },
   "source": [
    "# Scraping tweets containing keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8274c5",
   "metadata": {
    "id": "9c8274c5"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# !pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93a7c3",
   "metadata": {
    "id": "2c93a7c3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "def getFilteredTweets():\n",
    "    text_query = '(\"grasso\" OR \"grassa\" ' \\\n",
    "                 'OR \"ciccione\" ' \\\n",
    "                 'OR \"culone\" OR \"nano\" ' \\\n",
    "                 'OR \"nana\" OR \"obeso\" ' \\\n",
    "                 'OR \"pelata\" OR \"pelato\")'\n",
    "    since_date = '2022-06-01'\n",
    "    until_date = '2022-07-01'\n",
    "    options = '-is:retweet -is:reply -is:quoted lang:it'\n",
    "    output_file = './raw_Scraped/2022-06.csv'\n",
    "    tweets_list = []\n",
    "    fetched = 0\n",
    "\n",
    "    print(\"output_file: \", output_file)\n",
    "\n",
    "    for i, tweet in enumerate(\n",
    "            sntwitter.TwitterSearchScraper(\n",
    "                text_query + ' since:' + since_date + ' until:' + until_date + ' ' + options).get_items()):\n",
    "        dfk = pd.read_csv(\"keys.txt\", sep=';')\n",
    "\n",
    "        Words = dfk['Words'].values\n",
    "        for word in Words:\n",
    "            if word in tweet.content:\n",
    "\n",
    "                fetched = fetched+1\n",
    "\n",
    "                tweets_list.append(\n",
    "                    [tweet.date, tweet.content, tweet.user.username])\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Tweets obtained: \", fetched, \"\\t\\tdate: \", tweet.date)\n",
    "\n",
    "                break\n",
    "\n",
    "    tweets_df = pd.DataFrame(tweets_list,\n",
    "                             columns=['datetime', 'text', 'username'])\n",
    "    tweets_df.to_csv(output_file, index=False, sep=',')\n",
    "\n",
    "\n",
    "def scrape():\n",
    "    getFilteredTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086c6c0",
   "metadata": {
    "id": "5086c6c0"
   },
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c411904",
   "metadata": {
    "id": "8c411904"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge():\n",
    "    # setting the path for joining multiple files\n",
    "    files = os.path.join(\"./monitoring/\", \"*.csv\")\n",
    "\n",
    "    # list of merged files returned\n",
    "    files = glob.glob(files)\n",
    "\n",
    "    print(\"Resultant CSV after joining all CSV files at a particular location...\");\n",
    "\n",
    "    # joining files with concat and read_csv\n",
    "    df = pd.concat(map(pd.read_csv, files), ignore_index=True)\n",
    "    df.sort_values('datetime', inplace=True, ascending=True)\n",
    "    print(df.shape)\n",
    "\n",
    "    df.to_csv('./merged/monitoring.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb38ce",
   "metadata": {
    "id": "12bb38ce"
   },
   "source": [
    "## - Concat two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2dc052",
   "metadata": {
    "id": "9d2dc052"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv()\n",
    "data2 = pd.read_csv()\n",
    "\n",
    "concate_data = pd.concat([data1,data2])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3f5e3eef",
    "b0733939",
    "10ddf5f3",
    "8990d29c",
    "5086c6c0",
    "ec8f41d9",
    "12bb38ce",
    "8351db08",
    "ce292406",
    "94862698",
    "BjVW0mYBEMMH",
    "07dae0b5",
    "89bcca8d",
    "da62310e",
    "C45M-Rribw5C",
    "8149c9fb",
    "019da1ef",
    "d0baccbd",
    "9e9b5b97",
    "f0dde321",
    "_nADI-wV9VAS",
    "qcT4gSRf8GDf",
    "22294383",
    "QiQ2PJVj8qRJ",
    "h3PdYjVs-k2u"
   ],
   "name": "BSblocker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
